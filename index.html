<!DOCTYPE HTML>
<html lang="en"><head><meta http-equiv="Content-Type" content="text/html; charset=UTF-8">
  
  <!-- Global site tag (gtag.js) - Google Analytics -->
<script async src="https://www.googletagmanager.com/gtag/js?id=UA-177858501-1"></script>
<script>
  window.dataLayer = window.dataLayer || [];
  function gtag(){dataLayer.push(arguments);}
  gtag('js', new Date());

  gtag('config', 'UA-177858501-1');
</script>
  
  <title>Ayan Kumar Bhunia</title>
  
  <meta name="author" content="Ayan Kumar Bhunia">
  <meta name="viewport" content="width=device-width, initial-scale=1">

  <link rel="stylesheet" type="text/css" href="style.css">
  <link rel="stylesheet" type="text/css" href="stylesheet.css">
  <link rel="icon" type="image/png" href="images/seal_icon.png">
</head>



<body>
  <table style="width:100%;max-width:1000px;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
    <tr style="padding:0px">
      <td style="padding:0px">
        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
          <tr style="padding:0px">
            <td style="padding:2.5%;width:63%;vertical-align:middle">
              <p style="text-align:center">
                <name>Ayan Kumar Bhunia</name>
              </p>

              <p style="text-align:justify"><font size="3">
                I am a Doctor of Philosophy (PhD) student, focusing on Computer Vision and Deep Learning,
                at <a href="http://sketchx.eecs.qmul.ac.uk/" target="_blank"><font size="3">SketchX Lab.</font></a> of
                <a href="https://www.surrey.ac.uk/centre-vision-speech-signal-processing"target=" _blank"><font size="3">Centre for Vision,
                  Speech and Signal Processing (CVSSP)</font></a>, <a href="https://www.surrey.ac.uk/" target="_blank"><font size="3">
                University of Surrey</font></a>, England, United Kingdom.
                My primary supervisor is <a href="http://personal.ee.surrey.ac.uk/Personal/Y.Song/" target="_blank"><font size="3">
                Prof. Yi-Zhe Song</font></a>, and co-supervisors are
                <a href="http://personal.ee.surrey.ac.uk/Personal/T.Xiang/index.html" target="_blank">
                  <font size="3">Prof. Tao(Tony) Xiang</font></a> and <a href="https://www.surrey.ac.uk/people/yongxin-yang" target="_blank">
                  <font size="3">Dr. Yongxin Yang</font></a>.
                </font></p>
                <p align="justify"><font size="3">
                Prior to that, I worked as a full-time research assistant at the
                  <a href="https://imi.ntu.edu.sg/Pages/Home.aspx"
                     target="_blank"><font size="3">Institute for Media Innovation (IMI) Lab</font></a> of
                  <a href="https://www.ntu.edu.sg/Pages/home.aspx"
                     target="_blank"><font size="3">Nanyang Technological University (NTU)</font></a>, Singapore.
                </font></p>

              Top-venue Conference publications (July 2021):
               6x<b>CVPR</b>, 3x<b>ICCV</b>, 1x<b>ECCV</b>, 1x<b>Siggraph Asia</b>.


              <p style="text-align:center">
                <a href="https://scholar.google.co.in/citations?user=gjslbzsAAAAJ&hl=en" target="_blank"><font size="3">Google Scholar </font></a> &nbsp/&nbsp
                 <a href="https://github.com/AyanKumarBhunia" target="_blank"><font size="3"> GitHub </font></a> &nbsp/&nbsp
               <a href="https://www.linkedin.com/in/loansurrey/" target="_blank"><font size="3">  LinkedIn  </font></a> &nbsp/&nbsp
              <a href="https://dblp.org/pers/b/Bhunia:Ayan_Kumar.html" target="_blank"><font size="3">  DBLP  </font></a>
              </p>
            </td>

            <td style="padding:2.5%;width:40%;max-width:40%">
              <a href="images/ayannew_circle.png"><img style="width:100%;max-width:100%" alt="profile photo" src="images/ayannew_circle.png" class="hoverZoomLink"></a>

            </td>
          </tr>



        </tbody></table>

<!--        <table-->
<!--            style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;">-->
<!--            <tbody>-->
<!--              <tr>-->
<!--                <td style="padding:20px;width:100%;vertical-align:middle;text-align:justify">-->
<!--                  <heading>-->
<!--                      <font size="5">Recent Updates </font>-->
<!--                  </heading>-->
<!--                  <p>-->
<!--                    <font size="3">-->

<!--&lt;!&ndash;                  <p>&ndash;&gt;-->
<!--&lt;!&ndash;                       <a class="button"   style="color:#FFA500"><strong&ndash;&gt;-->
<!--&lt;!&ndash;                              style="font-size:16px">Recent Updates:</strong></a>&ndash;&gt;-->
<!--&lt;!&ndash;&lt;!&ndash;                            style="font-size:15px;padding-left:5px;">01/2021 : </strong>&ndash;&gt;&ndash;&gt;-->
<!--&lt;!&ndash;                         <p>&ndash;&gt;-->
<!--                    <a class="button"   style="color:#FFA500"><strong style="font-size:12px">New:</strong></a>-->
<!--                        <b><font size=3.5">July 2021:</font></b> <i><b>Three</b></i> papers got accepted in <b>ICCV</b> 2021. <br>-->
<!--                     &#8195 &#8194 &#8194  <b><font size=3.5">March 2021:</font></b> <i><b>Four</b></i> papers got accepted in <b>CVPR</b> 2021.-->

<!--                  </p>-->

<!--                  </font>-->
<!--                  </p>-->
<!--                </td>-->
<!--              </tr>-->
<!--            </tbody>-->
<!--          </table>-->

        <table
            style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;">
            <tbody>
              <tr>
                <td style="padding:20px;width:100%;vertical-align:middle;text-align:justify">
                  <heading>
                    <font size="5">Research Interests</font>
                  </heading>
                  <p>
                    <font size="3">
                      <p>
                      My research focus is broadly centered around <b>Computer Vision</b> and <b>Deep Learning</b>.
                      In particular, I work on reasoning from <b>Sparse Image Data</b> (e.g. <b>sketch</b> or
                      <b>handwriting</b>) and on how
                      <b>human drawing</b> or <b>sketch</b> could be used for various visual understanding tasks.
                      In context of sketch, I have
                      developed novel algorithms that can handle <b>partial data</b> for on-the-fly cross-modal retrieval.
                      I intend to explore deep model under <b>low-resource training data scenario</b> using <b>semi-supervised</b>,
                      <b>self-supervised</b> and <b>few-shot adaptive</b> paradigm. Furthermore, I earlier worked (occasionally work) on Document Image Analysis 
                      and Text Recognition related problems. 

                  <p>
                       <a class="button"   style="color:#FFA500"><strong style="font-size:12px">Notes:</strong></a>
                          <em style="font-size:16px;"> If you are interested in some potential research collaboration, feel free to contact me by Email or
                            <a href="https://www.linkedin.com/in/loansurrey/" target="_blank"><font size="3">  LinkedIn </font></a>. Most importantly, I would
                            be happy to collaborate with some really self-motivated and enthusiastic undergraduate or post-graduate
                            students who have intention to pursue MS/Ph.D. in future.
                  </p>

                  </font>
                  </p>
                </td>
              </tr>
            </tbody>


<!--            style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;">-->
            <tbody>
              <tr>
                <td style="padding:20px;width:100%;vertical-align:middle;text-align:justify">
                  <heading>
                      <font size="5">Recent Updates </font>
                  </heading>
                  <p>
                    <font size="3">

                        	<li><sup><font color="red"><strong>New!!</strong></font></sup> <font size=3.5"> [July 2021]:</font></b> <i><b>Three</b></i> papers got accepted in <b>ICCV</b> 2021!!</li>

	                        <li> <font size=3.5"> [June 2021]: </font></b> <i> Talk on <i>'Beyond Supervised Sketch Representation Learning'</i>
                            <a href="https://www.youtube.com/watch?v=dW9C4SUm9Iw" target="_blank"><font size="3">YouTube </font> </a> </li>

                        <li> <font size=3.5"> [March 2021] :</font></b> <i><b>Four</b></i> papers got accepted in <b>CVPR</b> 2021. </li>
                        <li> <font size=3.5"> [Aug 2020] :</font></b> <i><b>One</b></i> paper got accepted in <b>Siggraph Asia</b> 2020. Check <a href="http://surrey.ac:9999/" target="_blank"><font size="3">Online Demo </font> </a></li> 
                        <li> <font size=3.5"> [Aug 2020] :</font></b> <i><b>One</b></i> paper got accepted in <b>BMVC</b> 2020 for oral presentation.</li> 
                        <li> <font size=3.5"> [July 2020] :</font></b> <i><b>One</b></i> paper got accepted in <b>ECCV</b> 2020.</li> 
                        <li> <font size=3.5"> [March 2020] :</font></b> <i><b>One</b></i> paper got accepted in <b>CVPR</b> 2020 for <b>oral</b> presentation.</li> 

                           

<!--<a href="http://personal.ee.surrey.ac.uk/Personal/Y.Song/" target="_blank"><font size="3">-->
<!--                Dr. Yi-Zhe Song</font></a>-->
                  </p>

                  </font>
                  </p>
                </td>
              </tr>
            </tbody>


          </table>


<!--&lt;!&ndash; Updates on Recent Activities  &ndash;&gt;-->
<!--          <table-->
<!--            style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;">-->
<!--            <tbody>-->
<!--              <tr>-->
<!--                <td-->
<!--                  style="padding:20px;padding-top:5px;padding-bottom:5px;width:100%;vertical-align:middle;text-align:justify">-->
<!--                  <heading>-->
<!--                    <font size="5">Updates</font>-->
<!--                  </heading>-->
<!--                  <p>-->
<!--                    <font size="3">-->
<!--                      &lt;!&ndash; <a class="button" href="#" style="color:#FFA500"><strong style="font-size:12px">New</strong></a> &ndash;&gt;-->
<!--                      &lt;!&ndash; <button type="submit" class="button">Click me!</button> &ndash;&gt;-->
<!--                      <ul style="padding-inline-start:0px;list-style-type:none;">-->
<!--                        &lt;!&ndash;                         <li> <a class="button" href="#" style="color:#FFA500"><strong style="font-size:12px">New</strong></a><em style="font-size:16px;"> 1 paper on Future Frame Depth Prediction communicated to <strong style="font-size:16px">ECCV 2020</strong></em></li> &ndash;&gt;-->
<!--                        <p style="margin-block-start:0px;margin-block-end:3px"></p>-->
<!--                        <li> <a class="button" href="#" style="color:#FFA500"><strong-->
<!--                              style="font-size:12px">New</strong></a> <strong-->
<!--                            style="font-size:15px;padding-left:5px;">03/2021 : </strong>-->
<!--                          <em style="font-size:16px;"> My 1st PhD work has been communicated to a top-tier Computer Vision conference.-->

<!--                        <p style="margin-block-start:0px;margin-block-end:3px"></p>-->
<!--                        <li> <a class="button" href="#" style="color:#FFA500"><strong-->
<!--                              style="font-size:12px">New</strong></a> <strong-->
<!--                            style="font-size:15px;padding-left:5px;">01/2021 : </strong>-->
<!--                          <em style="font-size:16px;"> I am serving as a reviewer for <strong><a href="https://www.springer.com/journal/42979?gclid=Cj0KCQjwgtWDBhDZARIsADEKwgNEqQSbM5pYHdDB-Af_i4KITk24r5r8GgbSfwhvxYlRTaCtx8QLKIYaAswyEALw_wcB"-->
<!--                                style="font-size:16px" >Springer Computer Science </a> </strong> journal.-->

<!--                        <p style="margin-block-start:0px;margin-block-end:3px"></p>-->
<!--                        <li> <a class="button" href="#" style="color:#FFA500"><strong-->
<!--                              style="font-size:12px">New</strong></a> <strong-->
<!--                            style="font-size:15px;padding-left:5px;">09/2020 : </strong>-->
<!--                          <em style="font-size:16px;"> I have moved to South-West London to join<strong style="font-size:16px"> ReID-->
<!--                              Lab,-->
<!--                              University of Surrey</strong> as a PhD student.-->



<!--                        <p style="margin-block-start:0px;margin-block-end:3px"></p>-->
<!--                        <li> <a class="button" href="#" style="color:#FFA500"><strong-->
<!--                              style="font-size:12px">New</strong></a> <strong-->
<!--                            style="font-size:15px;padding-left:5px;">05/2020 : </strong>-->
<!--                          <em style="font-size:16px;"> <strong>1</strong> paper communicated to <strong-->
<!--                              style="font-size:16px"> <a-->
<!--                                href="https://ieeexplore.ieee.org/xpl/RecentIssue.jsp?punumber=76"-->
<!--                                style="font-size:16px">IEEE Transactions on Circuits and Systems for Video-->
<!--                                Technology</a> </strong></em></li>-->

<!--                        <p style="margin-block-start:0px;margin-block-end:3px"></p>-->
<!--                        <li> <a class="button" href="#" style="color:#FFA500"><strong-->
<!--                              style="font-size:12px">New</strong></a> <strong-->
<!--                            style="font-size:15px;padding-left:5px;">05/2020 : </strong>-->
<!--                          <em style="font-size:16px;"> <strong>1</strong> paper got accepted in <strong-->
<!--                              style="font-size:16px"> <a href="https://www.journals.elsevier.com/pattern-recognition"-->
<!--                                style="font-size:16px">Pattern Recognition, Elsevier</a> </strong></em></li>-->


<!--                        <p style="margin-block-start:0px;margin-block-end:3px"></p>-->
<!--                        <li style="padding-left:40px;">-->
<!--                          &lt;!&ndash; <a class="button" href="#" style="color:#FFA500"><strong-->
<!--                              style="font-size:12px">New</strong></a>  &ndash;&gt;-->
<!--                          <strong style="font-size:15px;padding-left:5px;">11/2019 : </strong>-->
<!--                          <em style="font-size:16px;"> Our paper titled-->
<!--                            "What's There in The Dark" has been selected in <strong style="font-size:16px"><a-->
<!--                                style="font-size:16px" href="http://2019.ieeeicip.org/?action=page3&id=9">TOP 10%-->
<!--                                Papers</a></strong> in <strong style="font-size:16px">ICIP19</strong></em></li>-->

<!--                        <p style="margin-block-start:0px;margin-block-end:3px"></p>-->
<!--                        <li style="padding-left:45px;"> <em style="font-size:16px;"></em> <strong-->
<!--                            style="font-size:15px">10/2019 : </strong> </em> <strong>1</strong> paper on <strong-->
<!--                            style="font-size:16px">Light-weight Saliency Detection</strong> is uploaded on Arxiv</em>-->
<!--                        </li>-->
<!--                      </ul>-->
<!--                    </font>-->
<!--                  </p>-->
<!--                </td>-->
<!--              </tr>-->
<!--            </tbody>-->
<!--          </table>-->



















<!--        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>-->
<!--            <tr>-->
<!--            <td style="padding:0px;width:100%;vertical-align:middle;text-align:justify">-->
<!--              <heading><font size="5">Research Interests</font></heading>-->
<!--              <p><font size="3">-->
<!--                I am broadly interested in the field of Computer Vision and Deep Learning. Particularly, I like to think upon <b>Visual Scene Understanding (VSU)</b> from images and videos, effective methods of <b>Domain Adaptation & Transfer Learning</b> for VSU, building <b>systems that learn with minimal or no supervision</b> and <b>systems that generalize well</b> in real and diverse scenarios.-->
<!--                I am also open to any topic that would be interesting or fun to explore and pursue.-->
<!--              </font></p>-->
<!--            </td>-->
<!--          </tr>-->
<!--        </tbody></table>-->

        <!-- PUBLICATIONS -->
        <table style="width:95%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
            <tr>
            <td style="padding:0px;width:100%;vertical-align:middle;text-align:justify">
               <p>
              </p>
              <heading><font size="5">Selected Publications</font></heading>
            </td>
          </tr>
        </tbody></table>

        <script>
          function myFunction(pub_name) {
              var x = document.getElementById(pub_name);
              if (x.style.display === 'none') {
                  x.style.display = 'block';
              } else {
                  x.style.display = 'none';
              }
        }
        </script>

                <table style="width:85%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
            <tr>
            <td style="padding:0px;width:100%;vertical-align:middle;text-align:justify">
               <p>
              </p>
              <heading><font size="5">2021</font></heading>
            </td>
          </tr>
        </tbody></table>

        <table style="width:80%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>


         <!-- ICCV - HANDWRITING RECOGNITION -->
          <tr onmouseout="ICCV21_3_stop()" onmouseover="ICCV21_3_start()">
            <td style="padding:20px;width:25%;vertical-align:middle">
              <div class="one">
                <div class="two" id='ICCV21_3_image'><img src='images/ICCV21_3_1.jpg'></div>
                <img src='images/ICCV21_3_1.jpg'>
              </div>
              <script type="text/javascript">
                function ICCV21_3_start() {
                  document.getElementById('ICCV21_3_image').style.opacity = "1";
                }

                function ICCV21_3_stop() {
                  document.getElementById('ICCV21_3_image').style.opacity = "0";
                }
                ICCV21_3_stop()
              </script>
            </td>
            <td style="padding:10px;width:75%;vertical-align:middle">
              <a href="https://openaccess.thecvf.com/content/ICCV2021/papers/Bhunia_Text_Is_Text_No_Matter_What_Unifying_Text_Recognition_Using_ICCV_2021_paper.pdf" target="_blank">
                <papertitle><font size="3">Text is Text, No Matter What: Unifying Text Recognition using Knowledge Distillation</font></papertitle>
              </a>
              <br><br>
               <strong><font size="3"> Ayan Kumar Bhunia</font></strong>, <font size="3">Aneeshan Sain</font>, <font size="3">Pinaki Nath Chowdhury</font>,
                <font size="3">Yi-Zhe Song</font> <font size="3"></font>.
              <br><font size="3">
                            <em>IEEE International Conference on Computer Vision (<strong><font size="3.5">ICCV</font></strong>)</em>,
              2021 <font color="red"><strong>(New!)</strong></font>
              </font>
              <br>
              <p></p>
              <a href="javascript:void(0);" onclick="myFunction('ICCV21_3_abs')"><font size="3">Abstract</font></a> /
             <a href="https://arxiv.org/abs/2107.12087" target="_blank" target="_blank"><font size="3">arXiv</font></a> /
              <a href="javascript:void(0);" onclick="myFunction('ICCV21_3_bib')"><font size="3">BibTex</font></a>
              <p></p>
              <div id="ICCV21_3_abs" style="display:none; text-align:justify;min-width:350px;"><font size="3">
                <em> Text recognition remains a fundamental and extensively researched topic in computer vision, largely owing to its wide array of commercial applications. The challenging nature of the very problem however dictated a fragmentation of research efforts: Scene Text Recognition (STR) that deals with text in everyday scenes, and Handwriting Text Recognition (HTR) that tackles hand-written text. In this paper, for the first time, we argue for their unification -- we aim for a single model that can compete favourably with two separate state-of-the-art STR and HTR models. We first show that cross-utilisation of STR and HTR models trigger significant performance drops due to differences in their inherent challenges. We then tackle their union by introducing a knowledge distillation (KD) based framework. This is however non-trivial, largely due to the variable-length and sequential nature of text sequences, which renders off-the-shelf KD techniques that mostly works with global fixed length data inadequate. For that, we propose three distillation losses all of which specifically designed to cope with the aforementioned unique characteristics of text recognition. Empirical evidence suggests that our proposed unified model performs on par with individual models, even surpassing them in certain cases. Ablative studies demonstrates that naive baselines such as a two-stage framework, and domain adaption/generalisation alternatives do not work as well, further verifying the appropriateness of our design.
              </font></div>
              <div id="ICCV21_3_bib" style="font-family:Courier;display:none;min-width:350px;"><font size="2">
                <br>
                @InProceedings{textistext,<br>
                author = {Ayan Kumar Bhunia and Aneeshan Sain and Pinaki Nath Chowdhury and Yi-Zhe Song},<br>
                title = {Text is Text, No Matter What: Unifying Text Recognition using Knowledge Distillation},<br>
                booktitle = {The IEEE International Conference on Computer Vision (ICCV)},<br>
                month = {October},<br>
                year = {2021}<br>
                }
              </font></div>
              </td>
          </tr>



               <!-- ICCV - HANDWRITING RECOGNITION -->
          <tr onmouseout="ICCV21_2_stop()" onmouseover="ICCV21_2_start()">
            <td style="padding:20px;width:25%;vertical-align:middle">
              <div class="one">
                <div class="two" id='ICCV21_2_image'><img src='images/ICCV21_2_1.jpg'></div>
                <img src='images/ICCV21_2_1.jpg'>
              </div>
              <script type="text/javascript">
                function ICCV21_2_start() {
                  document.getElementById('ICCV21_2_image').style.opacity = "1";
                }

                function ICCV21_2_stop() {
                  document.getElementById('ICCV21_2_image').style.opacity = "0";
                }
                ICCV21_2_stop()
              </script>
            </td>
            <td style="padding:10px;width:75%;vertical-align:middle">
              <a href="https://openaccess.thecvf.com/content/ICCV2021/papers/Bhunia_Towards_the_Unseen_Iterative_Text_Recognition_by_Distilling_From_Errors_ICCV_2021_paper.pdf" target="_blank">
                <papertitle><font size="3">Towards the Unseen: Iterative Text Recognition by Distilling from Errors</font></papertitle>
              </a>
              <br><br>
               <strong><font size="3"> Ayan Kumar Bhunia</font></strong>, <font size="3">Pinaki Nath Chowdhury</font>, <font size="3">Aneeshan Sain</font>,
              <font size="3">Yi-Zhe Song</font> <font size="3"></font>.
              <br><font size="3">
                            <em>IEEE International Conference on Computer Vision (<strong><font size="3.5">ICCV</font></strong>)</em>,
              2021 <font color="red"><strong>(New!)</strong></font>
              </font>
              <br>
              <p></p>
              <a href="javascript:void(0);" onclick="myFunction('ICCV21_2_abs')"><font size="3">Abstract</font></a> /
             <a href="https://arxiv.org/abs/2107.12081" target="_blank" target="_blank"><font size="3">arXiv</font></a> /
              <a href="javascript:void(0);" onclick="myFunction('ICCV21_2_bib')"><font size="3">BibTex</font></a>
              <p></p>
              <div id="ICCV21_2_abs" style="display:none; text-align:justify;min-width:350px;"><font size="3">
                <em> Visual text recognition is undoubtedly one of the most extensively researched topics in computer vision. Great progress have been made to date, with the latest models starting to focus on the more practical ``in-the-wild'' setting. However, a salient problem still hinders practical deployment -- prior arts mostly struggle with recognising unseen (or rarely seen) character sequences. In this paper, we put forward a novel framework to specifically tackle this ``unseen'' problem. Our framework is iterative in nature, in that it utilises predicted knowledge of character sequences from a previous iteration, to augment the main network in improving the next prediction. Key to our success is a unique cross-modal variational autoencoder to act as a feedback module, which is trained with the presence of textual error distribution data. This module importantly translate a discrete predicted character space, to a continuous affine transformation parameter space used to condition the visual feature map at next iteration. Experiments on common datasets have shown competitive performance over state-of-the-arts under the conventional setting. Most importantly, under the new disjoint setup where train-test labels are mutually exclusive, ours offers the best performance thus showcasing the capability of generalising onto unseen words.
              </font></div>
              <div id="ICCV21_2_bib" style="font-family:Courier;display:none;min-width:350px;"><font size="2">
                <br>
                @InProceedings{towardsunseen,<br>
                author = {Ayan Kumar Bhunia and Pinaki Nath Chowdhury and Aneeshan Sain and Yi-Zhe Song},<br>
                title = {Towards the Unseen: Iterative Text Recognition by Distilling from Errors},<br>
                booktitle = {The IEEE International Conference on Computer Vision (ICCV)},<br>
                month = {October},<br>
                year = {2021}<br>
                }
              </font></div>
              </td>
          </tr>








       <!-- ICCV - HANDWRITING RECOGNITION -->
          <tr onmouseout="ICCV21_1_stop()" onmouseover="ICCV21_1_start()">
            <td style="padding:20px;width:25%;vertical-align:middle">
              <div class="one">
                <div class="two" id='ICCV21_1_image'><img src='images/ICCV21_1_1.jpg'></div>
                <img src='images/ICCV21_1_1.jpg'>
              </div>
              <script type="text/javascript">
                function ICCV21_1_start() {
                  document.getElementById('ICCV21_1_image').style.opacity = "1";
                }

                function ICCV21_1_stop() {
                  document.getElementById('ICCV21_1_image').style.opacity = "0";
                }
                ICCV21_1_stop()
              </script>
            </td>
            <td style="padding:10px;width:75%;vertical-align:middle">
              <a href="https://openaccess.thecvf.com/content/ICCV2021/papers/Bhunia_Joint_Visual_Semantic_Reasoning_Multi-Stage_Decoder_for_Text_Recognition_ICCV_2021_paper.pdf" target="_blank">
                <papertitle><font size="3">Joint Visual Semantic Reasoning: Multi-Stage Decoder for Text Recognition</font></papertitle>
              </a>
              <br><br>
               <strong><font size="3"> Ayan Kumar Bhunia</font></strong>, <font size="3">Aneeshan Sain</font>,
               <font size="3">Amandeep Kumar</font>,  <font size="3">Shuvozit Ghose</font>, <font size="3">Pinaki Nath Chowdhury</font>,
              <font size="3">Yi-Zhe Song</font> <font size="3"></font>.
              <br><font size="3">
                            <em>IEEE International Conference on Computer Vision (<strong><font size="3.5">ICCV</font></strong>)</em>,
              2021 <font color="red"><strong>(New!)</strong></font>
              </font>
              <br>
              <p></p>
              <a href="javascript:void(0);" onclick="myFunction('ICCV21_1_abs')"><font size="3">Abstract</font></a> /
             <a href="https://arxiv.org/abs/2107.12090" target="_blank" target="_blank"><font size="3">arXiv</font></a> /
              <a href="javascript:void(0);" onclick="myFunction('ICCV21_1_bib')"><font size="3">BibTex</font></a>
              <p></p>
              <div id="ICCV21_1_abs" style="display:none; text-align:justify;min-width:350px;"><font size="3">
                <em> Although text recognition has significantly evolved over the years, state-of the-art (SOTA) models still struggle in the wild scenarios due to complex backgrounds, varying fonts, uncontrolled illuminations, distortions and other artifacts. This is because such models solely depend on visual information for text recognition, thus lacking semantic reasoning capabilities. In this paper, we argue that semantic information offers a complimentary role in addition to visual only. More specifically, we additionally utilize semantic information by proposing a multi-stage multi-scale attentional decoder that performs joint visual-semantic reasoning. Our novelty lies in the intuition that for text recognition, prediction should be refined in a stage-wise manner. Therefore our key contribution is in designing a stage-wise unrolling attentional decoder where non-differentiability, invoked by discretely predicted character labels, needs to be bypassed for end-to-end training. While the first stage predicts using visual features, subsequent stages refine on-top of it using joint visual-semantic information. Additionally, we introduce multi-scale 2D attention along with dense and residual connections between different stages to deal with varying scales of character sizes, for better performance and faster convergence during training. Experimental results show our approach to outperform existing SOTA methods by a considerable margin.
              </font></div>
              <div id="ICCV21_1_bib" style="font-family:Courier;display:none;min-width:350px;"><font size="2">
                <br>
                @InProceedings{JVSR,<br>
                author = {Ayan Kumar Bhunia and Aneeshan Sain and Amandeep Kumar and  Shuvozit Ghose and Pinaki Nath Chowdhury and Yi-Zhe Song},<br>
                title = {Joint Visual Semantic Reasoning: Multi-Stage Decoder for Text Recognition},<br>
                booktitle = {The IEEE International Conference on Computer Vision (ICCV)},<br>
                month = {October},<br>
                year = {2021}<br>
                }
              </font></div>
              </td>
          </tr>






       <!-- CVPR21 - HANDWRITING RECOGNITION -->
          <tr onmouseout="CVPR21_5_stop()" onmouseover="CVPR21_5_start()">
            <td style="padding:20px;width:25%;vertical-align:middle">
              <div class="one">
                <div class="two" id='CVPR21_5_image'><img src='images/CVPR21_5_1.JPG'></div>
                <img src='images/CVPR21_5_1.JPG'>
              </div>
              <script type="text/javascript">
                function CVPR21_5_start() {
                  document.getElementById('CVPR21_5_image').style.opacity = "1";
                }

                function CVPR21_5_stop() {
                  document.getElementById('CVPR21_5_image').style.opacity = "0";
                }
                CVPR21_5_stop()
              </script>
            </td>
            <td style="padding:10px;width:75%;vertical-align:middle">
              <a href="https://openaccess.thecvf.com/content/CVPR2021/html/Bhunia_Vectorization_and_Rasterization_Self-Supervised_Learning_for_Sketch_and_Handwriting_CVPR_2021_paper.html" target="_blank">
                <papertitle><font size="3">Vectorization and Rasterization: Self-Supervised Learning for Sketch and Handwriting</font></papertitle>
              </a>
              <br><br>
               <strong><font size="3"> Ayan Kumar Bhunia</font></strong>, <font size="3">Pinaki Nath Chowdhury</font>, <font size="3">Yongxin Yang</font>,  <font size="3">Timothy Hospedales</font>, <font size="3">Tao Xiang</font>,
              <font size="3">Yi-Zhe Song</font> <font size="3"></font>.
              <br><font size="3">
                            <em>IEEE Conference on Computer Vision and Pattern Recognition (<strong><font size="3.5">CVPR</font></strong>)</em>,
              2021 <font color="red"><strong></strong></font>
              </font>
              <br>
              <p></p>
              <a href="javascript:void(0);" onclick="myFunction('CVPR21_5_abs')"><font size="3">Abstract</font></a> /
             <a href="https://github.com/AyanKumarBhunia/Self-Supervised-Learning-for-Sketch" target="_blank"><font size="3">Code</font></a> /
             <a href="https://arxiv.org/abs/2103.13716" target="_blank" target="_blank"><font size="3">arXiv</font></a> /
              <a href="javascript:void(0);" onclick="myFunction('CVPR21_5_bib')"><font size="3">BibTex</font></a>
              <p></p>
              <div id="CVPR21_5_abs" style="display:none; text-align:justify;min-width:350px;"><font size="3">
                <em> Self-supervised learning has gained prominence due to its efficacy at learning powerful representations from unlabelled data that achieve excellent performance on many challenging downstream tasks. However supervision-free pre-text tasks are challenging to design and usually modality specific. Although there is a rich literature of self-supervised methods for either spatial (such as images) or temporal data (sound or text) modalities, a common pre-text task that benefits both modalities is largely missing. In this paper, we are interested in defining a self-supervised pre-text task for sketches and handwriting data. This data is uniquely characterised by its existence in dual modalities of rasterized images and vector coordinate sequences. We address and exploit this dual representation by proposing two novel cross-modal translation pre-text tasks for self-supervised feature learning: Vectorization and Rasterization. Vectorization learns to map image space to vector coordinates and rasterization maps vector coordinates to image space. We show that the our learned encoder modules benefit both raster-based and vector-based downstream approaches to analysing hand-drawn data. Empirical evidence shows that our novel pre-text tasks surpass existing single and multi-modal self-supervision methods.</em>
              </font></div>
              <div id="CVPR21_5_bib" style="font-family:Courier;display:none;min-width:350px;"><font size="2">
                <br>
                @InProceedings{sketch2vec,<br>
                author = {Ayan Kumar Bhunia and Pinaki Nath Chowdhury and Yongxin Yang and Timothy Hospedales and  Tao Xiang and Yi-Zhe Song},<br>
                title = {Vectorization and Rasterization: Self-Supervised Learning for Sketch and Handwriting},<br>
                booktitle = {The IEEE Conference on Computer Vision and Pattern Recognition (CVPR)},<br>
                month = {June},<br>
                year = {2021}<br>
                }
              </font></div>
              </td>
          </tr>





              <!-- CVPR21 - HANDWRITING RECOGNITION -->
          <tr onmouseout="CVPR21_4_stop()" onmouseover="CVPR21_4_start()">
            <td style="padding:20px;width:25%;vertical-align:middle">
              <div class="one">
                <div class="two" id='CVPR21_4_image'><img src='images/CVPR21_4_1.jpg'></div>
                <img src='images/CVPR21_4_1.jpg'>
              </div>
              <script type="text/javascript">
                function CVPR21_4_start() {
                  document.getElementById('CVPR21_4_image').style.opacity = "1";
                }

                function CVPR21_4_stop() {
                  document.getElementById('CVPR21_4_image').style.opacity = "0";
                }
                CVPR21_4_stop()
              </script>
            </td>
            <td style="padding:20px;width:75%;vertical-align:middle">
              <a href="https://openaccess.thecvf.com/content/CVPR2021/html/Bhunia_More_Photos_Are_All_You_Need_Semi-Supervised_Learning_for_Fine-Grained_CVPR_2021_paper.html" target="_blank">
                <papertitle><font size="3">More Photos are All You Need: Semi-Supervised Learning for Fine-Grained Sketch Based Image Retrieval</font></papertitle>
              </a>
              <br><br>
               <strong><font size="3"> Ayan Kumar Bhunia</font></strong>, <font size="3">Pinaki Nath Chowdhury</font>, <font size="3">Aneeshan Sain</font>, <font size="3">Yongxin Yang</font>,  <font size="3">Tao Xiang</font>,
              <font size="3">Yi-Zhe Song</font> <font size="3"></font>.
              <br><font size="3">
                            <em>IEEE Conference on Computer Vision and Pattern Recognition (<strong><font size="3.5">CVPR</font></strong>)</em>,
              2021 <font color="red"><strong></strong></font>
              </font>
              <br>
              <p></p>
              <a href="javascript:void(0);" onclick="myFunction('CVPR21_4_abs')"><font size="3">Abstract</font></a> /
             <a href="https://github.com/AyanKumarBhunia/semisupervised-FGSBIR" target="_blank"><font size="3">Code</font></a> /
             <a href="https://arxiv.org/abs/2103.13990" target="_blank" target="_blank"><font size="3">arXiv</font></a> /
              <a href="javascript:void(0);" onclick="myFunction('CVPR21_4_bib')"><font size="3">BibTex</font></a>
              <p></p>
              <div id="CVPR21_4_abs" style="display:none; text-align:justify;min-width:350px;"><font size="3">
                <em>A fundamental challenge faced by existing Fine-Grained Sketch-Based Image Retrieval (FG-SBIR) models is the data scarcity -- model performances are largely bottlenecked by the lack of sketch-photo pairs. Whilst the number of photos can be easily scaled, each corresponding sketch still needs to be individually produced. In this paper, we aim to mitigate such an upper-bound on sketch data, and study whether unlabelled photos alone (of which they are many) can be cultivated for performances gain. In particular, we introduce a novel semi-supervised framework for cross-modal retrieval that can additionally leverage large-scale unlabelled photos to account for data scarcity. At the centre of our semi-supervision design is a sequential photo-to-sketch generation model that aims to generate paired sketches for unlabelled photos. Importantly, we further introduce a discriminator guided mechanism to guide against unfaithful generation, together with a distillation loss based regularizer to provide tolerance against noisy training samples. Last but not least, we treat generation and retrieval as two conjugate problems, where a joint learning procedure is devised for each module to mutually benefit from each other. Extensive experiments show that our semi-supervised model yields significant performance boost over the state-of-the-art supervised alternatives, as well as existing methods that can exploit unlabelled photos for FG-SBIR.</em>
              </font></div>
              <div id="CVPR21_4_bib" style="font-family:Courier;display:none;min-width:350px;"><font size="2">
                <br>
                @InProceedings{semi-fgsbir,<br>
                author = {Ayan Kumar Bhunia and Pinaki Nath Chowdhury and Aneeshan Sain and Yongxin Yang and Tao Xiang and Yi-Zhe Song},<br>
                title = {More Photos are All You Need: Semi-Supervised Learning for Fine-Grained Sketch Based Image Retrieval},<br>
                booktitle = {The IEEE Conference on Computer Vision and Pattern Recognition (CVPR)},<br>
                month = {June},<br>
                year = {2021}<br>
                }
              </font></div>
              </td>
          </tr>


       <!-- sain beginning from here -->
       <!-- CVPR21 - STYLE-AGNOSTIC SBIR -->
          <tr onmouseout="CVPR21_3_stop()" onmouseover="CVPR21_3_start()">
            <td style="padding:20px;width:25%;vertical-align:middle">
              <div class="one">
                <div class="two" id='CVPR21_3_image'><img src='images/CVPR21_3_1.jpg'></div>
                <img src='images/CVPR21_3_1.jpg'>
              </div>
              <script type="text/javascript">
                function CVPR21_3_start() {
                  document.getElementById('CVPR21_3_image').style.opacity = "1";
                }
                function CVPR21_3_stop() {
                  document.getElementById('CVPR21_3_image').style.opacity = "0";
                }
                CVPR21_3_stop()
              </script>
            </td>
            <td style="padding:20px;width:75%;vertical-align:middle">
              <a href="https://openaccess.thecvf.com/content/CVPR2021/html/Sain_StyleMeUp_Towards_Style-Agnostic_Sketch-Based_Image_Retrieval_CVPR_2021_paper.html" target="_blank">
                <papertitle><font size="3">StyleMeUp: Towards Style-Agnostic Sketch-Based Image Retrieval</font></papertitle>
              </a>
              <br><br>
        <font size="3"> Aneeshan Sain</font>,
        <strong><font size="3">Ayan Kumar Bhunia</font></strong>,
        <font size="3">Yongxin Yang and </font>,
        <font size="3">Tao Xiang</font>,
        <font size="3">Yi-Zhe Song</font> <font size="3"></font>.
              <br><font size="3">
                <em>IEEE Conference on Computer Vision and Pattern Recognition (<strong><font size="3.5">CVPR</font></strong>)</em>,
              2021 <font color="red"><strong></strong></font>
              </font>
              <br>
              <p></p>
              <a href="javascript:void(0);" onclick="myFunction('CVPR21_3_abs')"><font size="3">Abstract</font></a> /
<!--              <a href="https://github.com/AyanKumarBhunia/on-the-fly-FGSBIR" target="_blank"><font size="3">Code</font></a> /-->
             <a href="https://arxiv.org/abs/2103.15706" target="_blank" target="_blank"><font size="3">arXiv</font></a> /
              <a href="javascript:void(0);" onclick="myFunction('CVPR21_3_bib')"><font size="3">BibTex</font></a>
              <p></p>
              <div id="CVPR21_3_abs" style="display:none; text-align:justify;min-width:350px;"><font size="3">
                <em>Sketch-based image retrieval (SBIR) is a cross-modal matching problem which is typically solved by learning a joint embedding space where the  semantic content shared between the photo and sketch modalities are preserved. However, a fundamental challenge in SBIR has been largely ignored so far, that is, sketches are drawn by humans and considerable style variations exist between different users. An effective SBIR model needs to explicitly account for this style diversity, and crucially to generalise to unseen user styles. To this end, a novel style-agnostic SBIR model is proposed. Different from existing models, a cross-modal variational autoencoder (VAE) is employed to explicitly disentangle each sketch into a semantic content part shared with the corresponding photo and a style part unique to the sketcher. Importantly, to make our model dynamically adaptable to any unseen user styles, we propose to meta-train our cross-modal VAE by adding two style-adaptive components: a set of feature transformation layers to its encoder and a regulariser to the disentangled semantic content latent code. With this meta-learning framework, our model can not only disentangle the cross-modal shared semantic content for SBIR, but can adapt the disentanglement to any unseen user styles as well, making the SBIR model truly style-agnostic. Extensive experiments show that our style-agnostic model yields state-of-the-art performance for both category-level and instance-level SBIR.</em>
              </font></div>
              <div id="CVPR21_3_bib" style="font-family:Courier;display:none;min-width:350px;"><font size="2">
                <br>
                @InProceedings{stylemeup,<br>
                author = {Aneeshan Sain and Ayan Kumar Bhunia and Yongxin Yang and Tao Xiang and Yi-Zhe Song},<br>
                title = {StyleMeUp: Towards Style-Agnostic Sketch-Based Image Retrieval},<br>
                booktitle = {The IEEE Conference on Computer Vision and Pattern Recognition (CVPR)},<br>
                month = {June},<br>
                year = {2021}<br>
                }
              </font></div>
              </td>
          </tr>



        <!-- CVPR21 - HANDWRITING RECOGNITION -->
          <tr onmouseout="CVPR21_0_stop()" onmouseover="CVPR21_0_start()">
            <td style="padding:20px;width:25%;vertical-align:middle">
              <div class="one">
                <div class="two" id='CVPR21_0_image'><img src='images/CVPR21_0_1.jpg'></div>
                <img src='images/CVPR21_0_1.jpg'>
              </div>
              <script type="text/javascript">
                function CVPR21_0_start() {
                  document.getElementById('CVPR21_0_image').style.opacity = "1";
                }

                function CVPR21_0_stop() {
                  document.getElementById('CVPR21_0_image').style.opacity = "0";
                }
                CVPR21_0_stop()
              </script>
            </td>
            <td style="padding:20px;width:75%;vertical-align:middle">
              <a href="https://openaccess.thecvf.com/content/CVPR2021/html/Bhunia_MetaHTR_Towards_Writer-Adaptive_Handwritten_Text_Recognition_CVPR_2021_paper.html" target="_blank">
                <papertitle><font size="3">MetaHTR: Towards Writer-Adaptive Handwritten Text Recognition</font></papertitle>
              </a>
              <br><br>
               <strong><font size="3"> Ayan Kumar Bhunia</font></strong>, <font size="3">Shuvozit Ghose</font>, <font size="3">Amandeep Kumar</font>, <font size="3">Pinaki Nath Chowdhury</font>, <font size="3">Aneeshan Sain</font>,
              <font size="3">Yi-Zhe Song</font> <font size="3"></font>.
              <br><font size="3">
                            <em>IEEE Conference on Computer Vision and Pattern Recognition (<strong><font size="3.5">CVPR</font></strong>)</em>,
              2021 <font color="red"><strong></strong></font>
              </font>
              <br>
              <p></p>
              <a href="javascript:void(0);" onclick="myFunction('cvpr21_0_abs')"><font size="3">Abstract</font></a> /
<!--              <a href="https://github.com/AyanKumarBhunia/on-the-fly-FGSBIR" target="_blank"><font size="3">Code</font></a> /-->
              <a href="https://arxiv.org/abs/2104.01876" target="_blank" target="_blank"><font size="3">arXiv</font></a> /
              <a href="javascript:void(0);" onclick="myFunction('cvpr21_0_bib')"><font size="3">BibTex</font></a>
              <p></p>
              <div id="cvpr21_0_abs" style="display:none; text-align:justify;min-width:350px;"><font size="3">
                <em>Handwritten Text Recognition (HTR) remains a challenging problem to date, largely due to the varying writing styles that exist amongst us. Prior works however generally operate with the assumption that there is a limited number of styles, most of which have already been captured by existing datasets. In this paper, we take a completely different perspective -- we work on the assumption that there is always a new style that is drastically different, and that we will only have very limited data during testing to perform adaptation. This results in a commercially viable solution -- the model has the best shot at adaptation being exposed to the new style, and the few samples nature makes it practical to implement. We achieve this via a novel meta-learning framework which exploits additional new-writer data through a support set, and outputs a writer-adapted model via single gradient step update, all during inference. We discover and leverage on the important insight that there exists few key characters per writer that exhibit relatively larger style discrepancies. For that, we additionally propose to meta-learn instance specific weights for a character-wise cross-entropy loss, which is specifically designed to work with the sequential nature of text data. Our writer-adaptive MetaHTR framework can be easily implemented on the top of most state-of-the-art HTR models. Experiments show an average performance gain of 5-7% can be obtained by observing very few new style data. We further demonstrate via a set of ablative studies the advantage of our meta design when compared with alternative adaption mechanisms.</em>
              </font></div>
              <div id="cvpr21_0_bib" style="font-family:Courier;display:none;min-width:350px;"><font size="2">
                <br>
                @InProceedings{metahtr,<br>
                author = {Ayan Kumar Bhunia and Shuvozit Ghose, Amandeep Kumar  and Pinaki Nath Chowdhury and Aneeshan Sain and Yi-Zhe Song},<br>
                title = {MetaHTR: Towards Writer-Adaptive Handwritten Text Recognition},<br>
                booktitle = {The IEEE Conference on Computer Vision and Pattern Recognition (CVPR)},<br>
                month = {June},<br>
                year = {2021}<br>
                }
              </font></div>
              </td>
          </tr>
        </tbody></table>






        <table style="width:85%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
            <tr>
            <td style="padding:0px;width:100%;vertical-align:middle;text-align:justify">
               <p>
              </p>
              <heading><font size="5">2020</font></heading>
            </td>
          </tr>
        </tbody></table>

        <table style="width:80%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
                   <!-- siggraphasia2020 - HANDWRITING RECOGNITION -->
          <tr onmouseout="siga2020_0_stop()" onmouseover="siga20_0_start()">
            <td style="padding:20px;width:25%;vertical-align:middle">
              <div class="one">
                <div class="two" id='siga2020_0_image'><img src='images/siga2020_1.jpg'></div>
                <img src='images/siga2020_2.jpg'>
              </div>
              <script type="text/javascript">
                function siga2020_0_start() {
                  document.getElementById('siga2020_0_image').style.opacity = "1";
                }

                function siga2020_0_stop() {
                  document.getElementById('siga2020_0_image').style.opacity = "0";
                }
                siga2020_0_stop()
              </script>
            </td>
            <td style="padding:20px;width:75%;vertical-align:middle">
              <a href="https://dl.acm.org/doi/pdf/10.1145/3414685.3417840" target="_blank">
                <papertitle><font size="3">Pixelor: A Competitive Sketching AI Agent. So you think you can beat me?</font></papertitle>
              </a>
              <br><br>
               <strong><font size="3"> Ayan Kumar Bhunia*</font></strong>, <font size="3">Ayan Das*</font>, <font size="3">Umar Riaz Muhammad*</font>, <font size="3">Yongxin Yang</font>, <font size="3">Timothy M. Hospedalis</font>,
              <font size="3">Tao Xiang</font>, <font size="3">Yulia Gryaditskaya</font>, <font size="3">Yi-Zhe Song</font> <font size="3"></font>.
              <br><font size="3">
              <em><strong><font size="3.5">SIGGRAPH Asia</font></strong></em>, 2020.

              </font>
              <br>
              <p></p>
              <a href="javascript:void(0);" onclick="myFunction('siga2020_0_abs')"><font size="3">Abstract</font></a> /
              <a href="https://github.com/AyanKumarBhunia/sketch-transformerMMD" target="_blank"><font size="3">Code</font></a> /
              <a href="https://epubs.surrey.ac.uk/858688/" target="_blank" target="_blank"><font size="3">arXiv</font></a> /
              <a href="javascript:void(0);" onclick="myFunction('siga2020_0_bib')"><font size="3">BibTex</font></a>/
             <a href="http://surrey.ac.:9999/" target="_blank"><font color="red"><strong>Try Online Demo</strong></font></a>
              <font>(*equal contribution)</font>
<!--              <font color="red"><strong>(Oral Presentation)</strong></font>-->
              <p></p>
              <div id="siga2020_0_abs" style="display:none; text-align:justify;min-width:350px;"><font size="3">
                <em> We present the first competitive drawing agent Pixelor that exhibits human-level performance at a Pictionary-like sketching game, where the participant whose sketch is recognized first is a winner. Our AI agent can autonomously sketch a given visual concept, and achieve a recognizable rendition as quickly or faster than a human competitor. The key to victory for the agent is to learn the optimal stroke sequencing strategies that generate the most recognizable and distinguishable strokes first. Training Pixelor is done in two steps. First, we infer the optimal stroke order that maximizes early recognizability of human training sketches. Second, this order is used to supervise the training of a sequence-to-sequence stroke generator. Our key technical contributions are a tractable search of the exponential space of orderings using neural sorting; and an improved Seq2Seq Wasserstein (S2S-WAE) generator that uses an optimal-transport loss to accommodate the multi-modal nature of the optimal stroke distribution. Our analysis shows that Pixelor is better than the human players of the Quick, Draw! game, under both AI and human judging of early recognition. To analyze the impact of human competitors’ strategies, we conducted a further human study with participants being given unlimited thinking time and training in early recognizability by feedback from an AI judge. The study shows that humans do gradually improve their strategies with training, but overall Pixelor still matches human performance. We will release the code and the dataset, optimized for the task of early recognition, upon acceptance.
                </em>
              </font></div>
              <div id="siga2020_0_bib" style="font-family:Courier;display:none;min-width:350px;"><font size="2">
                <br>
                @InProceedings{sketchxpixelor,<br>
                author = {Ayan Kumar Bhunia and Ayan Das and Umar Riaz Muhammad and Yongxin Yang and Timothy M. Hospedales and Tao Xiang and Yulia Gryaditskaya and Yi-Zhe Song},<br>
                title = {Pixelor: A Competitive Sketching AI Agent. So you think you can beat me?},<br>
                booktitle = {Siggraph Asia},<br>
                month = {November},<br>
                year = {2020}<br>
                }
              </font></div>
              </td>
          </tr>


                          <!-- BMVC2020 - HANDWRITING RECOGNITION -->
          <tr onmouseout="bmvc2020_0_stop()" onmouseover="bmvc20_0_start()">
            <td style="padding:20px;width:25%;vertical-align:middle">
              <div class="one">
                <div class="two" id='bmvc2020_0_image'><img src='images/BMVC2020_1.JPG'></div>
                <img src='images/BMVC2020_1.JPG'>
              </div>
              <script type="text/javascript">
                function bmvc2020_0_start() {
                  document.getElementById('bmvc2020_0_image').style.opacity = "1";
                }

                function bmvc2020_0_stop() {
                  document.getElementById('bmvc2020_0_image').style.opacity = "0";
                }
                bmvc2020_0_stop()
              </script>
            </td>
            <td style="padding:20px;width:75%;vertical-align:middle">
              <a href="https://arxiv.org/pdf/2007.15103.pdf" target="_blank">
                <papertitle><font size="3">Cross-Modal Hierarchical Modelling for Fine-Grained Sketch Based Image Retrieval</font></papertitle>
              </a>
              <br><br>
               <font size="3">Aneeshan Sain</font>, <strong><font size="3"> Ayan Kumar Bhunia</font></strong>, <font size="3">Yongxin Yang</font>,
              <font size="3">Tao Xiang</font>, <font size="3">Yi-Zhe Song</font> <font size="3"></font>.
              <br><font size="3">
              <em>British Machine Vision Conference (<strong><font size="3.5">BMVC </font></strong>)</em>, 2020.
              </font>
              <br>
              <p></p>
              <a href="javascript:void(0);" onclick="myFunction('bmvc2020_0_abs')"><font size="3">Abstract</font></a> /
<!--              <a href="https://github.com/AyanKumarBhunia/sketch-transformerMMD" target="_blank"><font size="3">Code</font></a> /-->
              <a href="https://arxiv.org/abs/2007.15103" target="_blank" target="_blank"><font size="3">arXiv</font></a> /
              <a href="javascript:void(0);" onclick="myFunction('bmvc2020_0_bib')"><font size="3">BibTex</font></a>
               <font><strong>(Oral Presentation)</strong></font>
              <p></p>
              <div id="bmvc2020_0_abs" style="display:none; text-align:justify;min-width:350px;"><font size="3">
                <em> Sketch as an image search query is an ideal alternative to text in capturing the fine-grained visual details. Prior successes on fine-grained sketch-based image retrieval (FG-SBIR) have demonstrated the importance of tackling the unique traits of sketches as opposed to photos, e.g., temporal vs. static, strokes vs. pixels, and abstract vs. pixel-perfect. In this paper, we study a further trait of sketches that has been overlooked to date, that is, they are hierarchical in terms of the levels of detail -- a person typically sketches up to various extents of detail to depict an object. This hierarchical structure is often visually distinct. In this paper,  we design a novel network that is capable of cultivating sketch-specific hierarchies and exploiting them to match sketch with photo at corresponding hierarchical levels. In particular, features from a sketch and a photo are enriched using cross-modal co-attention, coupled with hierarchical node fusion at every level to form a better embedding space to conduct retrieval. Experiments on common benchmarks show our method to outperform state-of-the-arts by a significant margin.
                </em>
              </font></div>
              <div id="bmvc2020_0_bib" style="font-family:Courier;display:none;min-width:350px;"><font size="2">
                <br>
                @InProceedings{sain2020crossmodal,<br>
                author = {Aneeshan Sain and Ayan Kumar Bhunia and Yongxin Yang and Tao Xiang and Yi-Zhe Song},<br>
                title = {Cross-Modal Hierarchical Modelling for Fine-Grained Sketch Based Image Retrieval},<br>
                booktitle = {BMVC},<br>
                month = {September},<br>
                year = {2020}<br>
                }
                </font></div>
              </td>
          </tr>

                      <!-- ECCV2020 - HANDWRITING RECOGNITION -->
          <tr onmouseout="eccv2020_0_stop()" onmouseover="bmvc20_0_start()">
            <td style="padding:20px;width:25%;vertical-align:middle">
              <div class="one">
                <div class="two" id='eccv2020_0_image'><img src='images/eccv2020_1.JPG'></div>
                <img src='images/eccv2020_1.JPG'>
              </div>
              <script type="text/javascript">
                function eccv2020_0_start() {
                  document.getElementById('eccv2020_0_image').style.opacity = "1";
                }

                function eccv2020_0_stop() {
                  document.getElementById('eccv2020_0_image').style.opacity = "0";
                }
                eccv2020_0_stop()
              </script>
            </td>
            <td style="padding:20px;width:75%;vertical-align:middle">
              <a href="http://www.ecva.net/papers/eccv_2020/papers_ECCV/papers/123650154.pdf" target="_blank">
                <papertitle><font size="3">Fine-grained visual classification via progressive multi-granularity training of jigsaw patches</font></papertitle>
              </a>
              <br><br>
               <font size="3">Ruoyi Du</font>, <font size="3">Dongliang Chang</font>, <strong><font size="3"> Ayan Kumar Bhunia</font></strong>, <font size="3">Jiyang Xie</font>,
              <font size="3">Zhanyu Ma</font>, <font size="3">Yi-Zhe Song</font> <font size="3"></font>, <font size="3">Jun Guo</font> <font size="3"></font>.
              <br><font size="3">
              <em>European Conference on Computer Vision (<strong><font size="3.5">ECCV </font></strong>)</em>, 2020.
              </font>
              <br>
              <p></p>
              <a href="javascript:void(0);" onclick="myFunction('eccv2020_0_abs')"><font size="3">Abstract</font></a> /
              <a href="https://github.com/PRIS-CV/PMG-Progressive-Multi-Granularity-Training" target="_blank"><font size="3">Code/</font></a>
              <a href="https://arxiv.org/pdf/2003.03836.pdf" target="_blank" target="_blank"><font size="3">arXiv</font></a> /
              <a href="javascript:void(0);" onclick="myFunction('eccv2020_0_bib')"><font size="3">BibTex</font></a>
              <p></p>
              <div id="eccv2020_0_abs" style="display:none; text-align:justify;min-width:350px;"><font size="3">
                <em> Fine-grained visual classiﬁcation (FGVC) is much more challenging than traditional classiﬁcation tasks due to the inherently subtle intra-class object variations. Recent works are mainly part-driven (either explicitly or implicitly), with the assumption that fine-grained information naturally rests within the parts. In this paper, we take a different stance, and show that part operations are not strictly necessary -- the key lies with encouraging the network to learn at different granularities and progressively fusing multi-granularity features together. In particular, we propose: (i) a progressive training strategy that effectively fuses features from different granularities, and (ii) a random jigsaw patch generator that encourages the network to learn features at specific granularities. We evaluate on several standard FGVC benchmark datasets, and show the proposed method consistently outperforms existing alternatives or delivers competitive results.</em>
              </font></div>
              <div id="eccv2020_0_bib" style="font-family:Courier;display:none;min-width:350px;"><font size="2">
                <br>
                @InProceedings{du2020fine,<br>
                author = {Du, Ruoyi and Chang, Dongliang and Bhunia, Ayan Kumar and Xie, Jiyang and Song, Yi-Zhe and Ma, Zhanyu and Guo, Jun},<br>
                title = {Fine-grained visual classification via progressive multi-granularity training of jigsaw patches},<br>
                booktitle = {ECCV},<br>
                month = {August},<br>
                year = {2020}<br>
                }
                </font></div>
              </td>
          </tr>

                   



                  <!-- CVPR 2019 - HANDWRITING RECOGNITION -->
          <tr onmouseout="cvpr20_0_stop()" onmouseover="cvpr20_0_start()">
            <td style="padding:20px;width:25%;vertical-align:middle">
              <div class="one">
                <div class="two" id='cvpr20_0_image'><img src='images/cvpr2020_1.jpg'></div>
                <img src='images/cvpr2020_2.jpg'>
              </div>
              <script type="text/javascript">
                function cvpr20_0_start() {
                  document.getElementById('cvpr20_0_image').style.opacity = "1";
                }

                function cvpr20_0_stop() {
                  document.getElementById('cvpr20_0_image').style.opacity = "0";
                }
                cvpr20_0_stop()
              </script>
            </td>
            <td style="padding:20px;width:75%;vertical-align:middle">
              <a href="https://openaccess.thecvf.com/content_CVPR_2020/html/Bhunia_Sketch_Less_for_More_On-the-Fly_Fine-Grained_Sketch-Based_Image_Retrieval_CVPR_2020_paper.html" target="_blank">
                <papertitle><font size="3">Sketch Less for More: On-the-Fly Fine-Grained Sketch Based Image Retrieval</font></papertitle>
              </a>
              <br><br>
               <strong><font size="3"> Ayan Kumar Bhunia</font></strong>, <font size="3">Yongxin Yang</font>, <font size="3">Timothy M. Hospedalis</font>,
              <font size="3">Tao Xiang</font>, <font size="3">Yi-Zhe Song</font>.
              <br><font size="3">
              <em>IEEE Conference on Computer Vision and Pattern Recognition (<strong><font size="3.5">CVPR</font></strong>)</em>, 2020.

              </font>
              <br>
              <p></p>
              <a href="javascript:void(0);" onclick="myFunction('cvpr20_0_abs')"><font size="3">Abstract</font></a> /
              <a href="https://github.com/AyanKumarBhunia/on-the-fly-FGSBIR" target="_blank"><font size="3">Code</font></a> /
              <a href="https://arxiv.org/abs/2002.10310" target="_blank" target="_blank"><font size="3">arXiv</font></a> /
              <a href="javascript:void(0);" onclick="myFunction('cvpr20_0_bib')"><font size="3">BibTex</font></a>
              <font color="red"><strong>(Oral Presentation)</strong></font>
              <p></p>
              <div id="cvpr20_0_abs" style="display:none; text-align:justify;min-width:350px;"><font size="3">
                <em>Fine-grained sketch-based image retrieval (FG-SBIR) addresses the problem of retrieving a particular photo instance given a user's query sketch. Its widespread applicability is however hindered by the fact that drawing a sketch takes time, and most people struggle to draw a complete and faithful sketch. In this paper, we reformulate the conventional FG-SBIR framework to tackle these challenges, with the ultimate goal of retrieving the target photo with the least number of strokes possible. We further propose an on-the-fly design that starts retrieving as soon as the user starts drawing. To accomplish this, we devise a reinforcement learning based cross-modal retrieval framework that directly optimizes rank of the ground-truth photo over a complete sketch drawing episode. Additionally, we introduce a novel reward scheme that circumvents the problems related to irrelevant sketch strokes, and thus provides us with a more consistent rank list during the retrieval. We achieve superior early-retrieval efficiency over state-of-the-art methods and alternative baselines on two publicly available fine-grained sketch retrieval datasets.
                </em>
              </font></div>
              <div id="cvpr20_0_bib" style="font-family:Courier;display:none;min-width:350px;"><font size="2">
                <br>
                @InProceedings{bhunia2020sketch,<br>
                author = {Ayan Kumar Bhunia and Yongxin Yang and Timothy M. Hospedales and Tao Xiang and Yi-Zhe Song},<br>
                title = {Sketch Less for More: On-the-Fly Fine-Grained Sketch Based Image Retrieval},<br>
                booktitle = {The IEEE Conference on Computer Vision and Pattern Recognition (CVPR)},<br>
                month = {June},<br>
                year = {2020}<br>
                }
              </font></div>
              </td>
          </tr>

         </tbody></table>

        <table style="width:85%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
            <tr>
            <td style="padding:0px;width:100%;vertical-align:middle;text-align:justify">
               <p>
              </p>
              <heading><font size="5">2019</font></heading>
            </td>
          </tr>
        </tbody></table>

        <table style="width:80%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>

          <!-- CVPR 2019 - HANDWRITING RECOGNITION -->
          <tr onmouseout="cvpr19_0_stop()" onmouseover="cvpr19_0_start()">
            <td style="padding:20px;width:25%;vertical-align:middle">
              <div class="one">
                <div class="two" id='cvpr19_0_image'><img src='images/cvpr2019_2.JPG'></div>
                <img src='images/cvpr2019_1.JPG'>
              </div>
              <script type="text/javascript">
                function cvpr19_0_start() {
                  document.getElementById('cvpr19_0_image').style.opacity = "1";
                }

                function cvpr19_0_stop() {
                  document.getElementById('cvpr19_0_image').style.opacity = "0";
                }
                cvpr19_0_stop()
              </script>
            </td>
            <td style="padding:20px;width:75%;vertical-align:middle">
              <a href="http://openaccess.thecvf.com/content_CVPR_2019/html/Bhunia_Handwriting_Recognition_in_Low-Resource_Scripts_Using_Adversarial_Learning_CVPR_2019_paper.html" target="_blank">
                <papertitle><font size="3">Handwriting Recognition in Low-Resource Scripts Using Adversarial Learning</font></papertitle>
              </a>
              <br><br>
               <strong><font size="3"> Ayan Kumar Bhunia </font></strong>, <font size="3">Abhirup Das</font>, <font size="3">Ankan Kumar Bhunia</font>,
              <font size="3">Perla Sai Raj Kishore</font>, <font size="3">Partha Pratim Roy</font>.
              <br><font size="3">
              <em>IEEE Conference on Computer Vision and Pattern Recognition (<strong><font size="3.5">CVPR</font></strong>)</em>, 2019
              </font>
              <br>
              <p></p>
              <a href="javascript:void(0);" onclick="myFunction('cvpr19_0_abs')"><font size="3">Abstract</font></a> /
              <a href="https://github.com/AyanKumarBhunia/Handwriting_Recogition_using_Adversarial_Learning" target="_blank"><font size="3">Code</font></a> /
              <a href="https://arxiv.org/abs/1811.01396" target="_blank" target="_blank"><font size="3">arXiv</font></a> /
              <a href="javascript:void(0);" onclick="myFunction('cvpr19_0_bib')"><font size="3">BibTex</font></a>
              <p></p>
              <div id="cvpr19_0_abs" style="display:none; text-align:justify;min-width:350px;"><font size="3">
                <em>
                  Handwritten Word Recognition and Spotting is a challenging field dealing with handwritten text possessing irregular and complex shapes. The design of deep neural network models makes it necessary to extend training datasets in order to introduce variations and increase the number of samples; word-retrieval is therefore very difficult in low-resource scripts. Much of the existing literature comprises preprocessing strategies which are seldom sufficient to cover all possible variations. We propose an Adversarial Feature Deformation Module (AFDM) that learns ways to elastically warp extracted features in a scalable manner. The AFDM is inserted between intermediate layers and trained alternatively with the original framework, boosting its capability to better learn highly informative features rather than trivial ones. We test our meta-framework, which is built on top of popular word-spotting and word-recognition frameworks and enhanced by AFDM, not only on extensive Latin word datasets but also on sparser Indic scripts. We record results for varying sizes of training data, and observe that our enhanced network generalizes much better in the low-data regime; the overall word-error rates and mAP scores are observed to improve as well.
                </em>
              </font></div>
              <div id="cvpr19_0_bib" style="font-family:Courier;display:none;min-width:350px;"><font size="2">
                <br>
                @InProceedings{Bhunia_2019_CVPR,<br>
                author = {Bhunia, Ayan Kumar and Das, Abhirup and Bhunia, Ankan Kumar and Kishore, Perla Sai Raj and Roy, Partha Pratim},<br>
                title = {Handwriting Recognition in Low-Resource Scripts Using Adversarial Learning},<br>
                booktitle = {The IEEE Conference on Computer Vision and Pattern Recognition (CVPR)},<br>
                month = {June},<br>
                year = {2019}<br>
                }
              </font></div>
              </td>
          </tr>


          <!-- ICIP 2019 - Unsupervised Binarization -->
          <tr onmouseout="ICIP19_0_stop()" onmouseover="ICIP19_0_start()">
            <td style="padding:0px;width:25%;vertical-align:middle">
              <div class="one">
                <div class="two" id='ICIP19_0_image'><img src='images/ICIP219_1.JPG'></div>
                <img src='images/ICIP219_2.JPG'>
              </div>
              <script type="text/javascript">
                function ICIP19_0_start() {
                  document.getElementById('ICIP19_0_image').style.opacity = "1";
                }
                function ICIP19_0_stop() {
                  document.getElementById('ICIP19_0_image').style.opacity = "0";
                }
                ICIP19_0_stop()
              </script>
            </td>
            <td style="padding:20px;width:75%;vertical-align:middle">
              <a href="https://ieeexplore.ieee.org/abstract/document/8803348/" target="_blank">
                <papertitle><font size="3">Improving Document Binarization via Adversarial Noise-Texture Augmentation</font></papertitle>
              </a>
              <br><br>
              <font size="3">Ankan Kumar Bhunia</font>, <strong><font size="3"> Ayan Kumar Bhunia </font></strong>,
              <font size="3">Aneeshan Sain</font>, <font size="3">Partha Pratim Roy</font>.
              <br><font size="3">
              <em>IEEE Conference on Image Processing (<strong><font size="3.5">ICIP</font></strong>)</em>, 2019
              </font>
              <br>
              <p></p>
              <a href="javascript:void(0);" onclick="myFunction('ICIP19_0_abs')"><font size="3">Abstract</font></a> /
              <a href="https://github.com/ankanbhunia/AdverseBiNet" target="_blank"><font size="3">Code</font></a> /
              <a href="https://arxiv.org/abs/1810.11120" target="_blank" target="_blank"><font size="3">arXiv</font></a> /
              <a href="javascript:void(0);" onclick="myFunction('ICIP19_0_bib')"><font size="3">BibTex</font></a>
              <strong>(Top 10% Papers)</strong>
              <p></p>
              <div id="ICIP19_0_abs" style="display:none; text-align:justify;min-width:350px;"><font size="3">
                <em>
                  Binarization of degraded document images is an elementary step in most of the problems in document image analysis domain. The paper re-visits the binarization problem by introducing an adversarial learning approach. We construct a Texture Augmentation Network that transfers the texture element of a degraded reference document image to a clean binary image. In this way, the network creates multiple versions of the same textual content with various noisy textures, thus enlarging the available document binarization datasets. At last, the newly generated images are passed through a Binarization network to get back the clean version. By jointly training the two networks we can increase the adversarial robustness of our system. Also, it is noteworthy that our model can learn from unpaired data. Experimental results suggest that the proposed method achieves superior performance over widely used DIBCO datasets.
                </em>
              </font></div>
              <div id="ICIP19_0_bib" style="font-family:Courier;display:none;min-width:350px;"><font size="2">
                <br>
                @inproceedings{bhunia2019improving, <br>
                title={Improving document binarization via adversarial noise-texture augmentation},
                author={Bhunia, Ankan Kumar and Bhunia, Ayan Kumar and Sain, Aneeshan and Roy, Partha Pratim},
                booktitle={2019 IEEE International Conference on Image Processing (ICIP)},
                pages={2721--2725},<br>
                year={2019},<br>
                organization={IEEE}
                }
              </font></div>
              </td>
          </tr>


          <!-- PR 2019 - One-shot Logo -->
          <tr onmouseout="PR19_0_stop()" onmouseover="PR19_0_start()">
            <td style="padding:0px;width:25%;vertical-align:middle">
              <div class="one">
                <div class="two" id='PR19_0_image'><img src='images/PR19_1.JPG'></div>
                <img src='images/PR19_2.JPG'>
              </div>
              <script type="text/javascript">
                function PR19_0_start() {
                  document.getElementById('PR19_0_image').style.opacity = "1";
                }
                function PR19_0_stop() {
                  document.getElementById('PR19_0_image').style.opacity = "0";
                }
                PR19_0_stop()
              </script>
            </td>
            <td style="padding:20px;width:75%;vertical-align:middle">
              <a href="https://www.sciencedirect.com/science/article/pii/S0031320319302626?dgcid=rss_sd_all" target="_blank">
                <papertitle><font size="3">A Deep One-Shot Network for Query-based Logo Retrieval</font></papertitle>
              </a>
              <br><br>
               <strong><font size="3"> Ayan Kumar Bhunia </font></strong>, <font size="3">Ankan Kumar Bhunia</font>,
              <font size="3">Shuvozit Ghose</font>, <font size="3">Abhirup Das</font>, <font size="3">Partha Pratim Roy</font>, <font size="3">Umapada Pal</font>
              <br><font size="3">
              <em>Pattern Recognition (<strong><font size="3.5">PR</font></strong>)</em>, 2019
              </font>
              <br>
              <p></p>
              <a href="javascript:void(0);" onclick="myFunction('PR19_0_abs')"><font size="3">Abstract</font></a> /
              <a href="https://github.com/AyanKumarBhunia/Deep-One-Shot-Logo-Retrieval" target="_blank"><font size="3">Code</font></a> /
              <a href="https://github.com/giovanniguidi/logo-detection" target="_blank"><font size="3">Third Party Implementation</font></a> /
              <a href="https://arxiv.org/abs/1811.01395" target="_blank" target="_blank"><font size="3">arXiv</font></a> /
              <a href="javascript:void(0);" onclick="myFunction('PR19_0_bib')"><font size="3">BibTex</font></a>
              <p></p>
              <div id="PR19_0_abs" style="display:none; text-align:justify;min-width:350px;"><font size="3">
                <em>
                  Logo detection in real-world scene images is an important problem with applications in advertisement and marketing. Existing general-purpose object detection methods require large training data with annotations for every logo class. These methods do not satisfy the incremental demand of logo classes necessary for practical deployment since it is practically impossible to have such annotated data for new unseen logo. In this work, we develop an easy-to-implement query-based logo detection and localization system by employing a one-shot learning technique using off the shelf neural network components. Given an image of a query logo, our model searches for logo within a given target image and predicts the possible location of the logo by estimating a binary segmentation mask. The proposed model consists of a conditional branch and a segmentation branch. The former gives a conditional latent representation of the given query logo which is combined with feature maps of the segmentation branch at multiple scales in order to obtain the matching location of the query logo in a target image. Feature matching between the latent query representation and multi-scale feature maps of segmentation branch using simple concatenation operation followed by 1 × 1 convolution layer makes our model scale-invariant. Despite its simplicity, our query-based logo retrieval framework achieved superior performance in FlickrLogos-32 and TopLogos-10 dataset over different existing baseline methods.
                </em>
              </font></div>
              <div id="PR19_0_bib" style="font-family:Courier;display:none;min-width:350px;"><font size="2">
                <br>
                @article{bhunia2019deep,
                  title={A Deep One-Shot Network for Query-based Logo Retrieval}, <br>
                  author={Bhunia, Ayan Kumar and Bhunia, Ankan Kumar and Ghose, Shuvozit and Das, Abhirup and Roy, Partha Pratim and Pal, Umapada},
                  journal={Pattern Recognition},
                  pages={106965},<br>
                  year={2019},<br>
                  publisher={Elsevier}}
              </font></div>
              </td>
          </tr>

          <!-- ICASSP 2019 - THUMBNAIL GENERATION -->
          <tr onmouseout="icassp19_0_stop()" onmouseover="icassp19_0_start()">
            <td style="padding:20px;width:25%;vertical-align:middle">
              <div class="one">
                <div class="two" id='icassp19_0_image'><img src='images/icassp19_0_1.jpg'></div>
                <img src='images/icassp19_0_2.jpg'>
              </div>
              <script type="text/javascript">
                function icassp19_0_start() {
                  document.getElementById('icassp19_0_image').style.opacity = "1";
                }

                function icassp19_0_stop() {
                  document.getElementById('icassp19_0_image').style.opacity = "0";
                }
                icassp19_0_stop()
              </script>
            </td>
            <td style="padding:20px;width:75%;vertical-align:middle">
              <a href="https://ieeexplore.ieee.org/document/8683761" target="_blank">
                <papertitle><font size="3">User Constrained Thumbnail Generation Using Adaptive Convolutions</font></papertitle>
              </a>
              <br><br>
              <font size="3">
              <font size="3">Perla Sai Raj Kishore</font>,
                <strong><font size="3">Ayan Kumar Bhunia</font></strong>, <font size="3">Shovozit Ghose</font>,
                <font size="3">Partha Pratim Roy</font>
              <br>
              <em>International Conference on Acoustics, Speech and Signal Processing (<strong><font size="3.5">ICASSP</font></strong>)</em>, 2019
              <br>

              </font>
              <br>
              <p></p>
              <a href="javascript:void(0);" onclick="myFunction('icassp19_0_abs')"><font size="3">Abstract</font></a> /
              <a href="https://github.com/sairajk/Thumbnail-Generation" target="_blank"><font size="3">Code</font></a> /
              <a href="https://arxiv.org/abs/1810.13054" target="_blank"><font size="3">arXiv</font></a> /
              <a href="javascript:void(0);" onclick="myFunction('icassp19_0_bib')"><font size="3">BibTex</font></a>
              <font color="black" size="3"><strong>(Oral Presentation)</strong></font>
              <p></p>
              <div id="icassp19_0_abs" style="display:none; text-align:justify;min-width:350px;"><font size="3">
                <em>
                  Thumbnails are widely used all over the world as a preview for digital images. In this work we propose a deep neural framework to generate thumbnails of any size and aspect ratio, even for unseen values during training, with high accuracy and precision. We use Global Context Aggregation (GCA) and a modified Region Proposal Network (RPN) with adaptive convolutions to generate thumbnails in real time. GCA is used to selectively attend and aggregate the global context information from the entire image while the RPN is used to generate candidate bounding boxes for the thumbnail image. Adaptive convolution eliminates the difficulty of generating thumbnails of various aspect ratios by using filter weights dynamically generated from the aspect ratio information. The experimental results indicate the superior performance of the proposed model 1 over existing state-of-the-art techniques.
                </em>
              </font></div>
              <div id="icassp19_0_bib" style="font-family:Courier;display:none;min-width:350px;"><font size="2">
                <br>
                @inproceedings{kishore2019user,<br>
                  title={User Constrained Thumbnail Generation Using Adaptive Convolutions},
                  author={Kishore, Perla Sai Raj and Bhunia, Ayan Kumar and Ghose, Shuvozit and Roy, Partha Pratim},<br>
                  booktitle={ICASSP 2019-2019 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)},<br>
                  pages={1677--1681},<br>
                  year={2019},<br>
                  organization={IEEE}<br>
                }
              </font></div>
            </td>
          </tr>


          <!-- WACV 2019 - TEXTURE PAPER -->
          <tr onmouseout="wacv19_0_stop()" onmouseover="wacv19_0_start()">
            <td style="padding:20px;width:25%;vertical-align:middle">
              <div class="one">
                <div class="two" id='wacv19_0_image'><img src='images/wacv19_0_1.JPG'></div>
                <img src='images/wacv19_0_2.JPG'>
              </div>
              <script type="text/javascript">
                function wacv19_0_start() {
                  document.getElementById('wacv19_0_image').style.opacity = "1";
                }

                function wacv19_0_stop() {
                  document.getElementById('wacv19_0_image').style.opacity = "0";
                }
                wacv19_0_stop()
              </script>
            </td>
            <td style="padding:20px;width:75%;vertical-align:middle">
              <a href="https://ieeexplore.ieee.org/abstract/document/8659343" target="_blank">
                <papertitle><font size="3">Texture Synthesis Guided Deep Hashing for Texture Image Retrieval</font></papertitle>
              </a>
             <br><br>
              <font size="3">
              <strong><font size="3">Ayan Kumar Bhunia</font></strong>,
              <font size="3">Perla Sai Raj Kishore</font>,
              <font size="3">Pranay Mukherjee</font>,
              <font size="3">Abhirup Das</font>,
              <font size="3">Partha Pratim Roy</font>
              <br>
              <em>IEEE Winter Conference on Applications of Computer Vision (<strong><font size="3.5">WACV</font></strong>)</em>, 2019
              </font>
              <br>
              <p></p>
              <a href="javascript:void(0);" onclick="myFunction('wacv19_0_abs')"><font size="3">Abstract</font></a> /
              <a href="https://arxiv.org/abs/1811.01401" target="_blank"><font size="3">arXiv</font></a> /
              <a href="javascript:void(0);" onclick="myFunction('wacv19_0_bib')"><font size="3">BibTex</font></a>/
              <a href="https://www.youtube.com/watch?v=tXaXTGhzaJo" target="_blank"><font size="3">Video Presentation</font></a>
              <p></p>
              <div id="wacv19_0_abs" style="display:none; text-align:justify;min-width:350px;"><font size="3">
                <em>
                  With the large scale explosion of images and videos over the internet, efficient hashing methods have been developed to facilitate memory and time efficient retrieval of similar images. However, none of the existing works use hashing to address texture image retrieval mostly because of the lack of sufficiently large texture image databases. Our work addresses this problem by developing a novel deep learning architecture that generates binary hash codes for input texture images. For this, we first pre-train a Texture Synthesis Network (TSN) which takes a texture patch as input and outputs an enlarged view of the texture by injecting newer texture content. Thus it signifies that the TSN encodes the learnt texture specific information in its intermediate layers. In the next stage, a second network gathers the multi-scale feature representations from the TSN’s intermediate layers using channel-wise attention, combines them in a progressive manner to a dense continuous representation which is finally converted into a binary hash code with the help of individual and pairwise label information. The new enlarged texture patches from the TSN also help in data augmentation to alleviate the problem of insufficient texture data and are used to train the second stage of the network. Experiments on three public texture image retrieval datasets indicate the superiority of our texture synthesis guided hashing approach over existing state-of-the-art methods.
                </em>
              </font></div>
              <div id="wacv19_0_bib" style="font-family:Courier;display:none;min-width:350px;"><font size="2">
                <br>
                @inproceedings{bhunia2019texture,<br>
                  title={Texture synthesis guided deep hashing for texture image retrieval},<br>
                  author={Bhunia, Ayan Kumar and Perla, Sai Raj Kishore and Mukherjee, Pranay and Das, Abhirup and Roy, Partha Pratim},<br>
                  booktitle={2019 IEEE Winter Conference on Applications of Computer Vision (WACV)},<br>
                  pages={609--618},<br>
                  year={2019},<br>
                  organization={IEEE}<br>
                }
              </font></div>
            </td>
          </tr>


          <!-- PR 2019 - Script Identification -->
          <tr onmouseout="PR19_1_stop()" onmouseover="PR19_1_start()">
            <td style="padding:0px;width:25%;vertical-align:middle">
              <div class="one">
                <div class="two" id='PR19_1_image'><img src='images/PR19_1_1.JPG'></div>
                <img src='images/PR19_1_2.JPG'>
              </div>
              <script type="text/javascript">
                function PR19_1_start() {
                  document.getElementById('PR19_1_image').style.opacity = "1";
                }
                function PR19_1_stop() {
                  document.getElementById('PR19_1_image').style.opacity = "0";
                }
                PR19_1_stop()
              </script>
            </td>
            <td style="padding:20px;width:75%;vertical-align:middle">
              <a href="https://www.sciencedirect.com/science/article/pii/S0031320318302590" target="_blank">
                <papertitle><font size="3">Script identification in natural scene image and video frames using an attention based Convolutional-LSTM network</font></papertitle>
              </a>
              <br>
              <br>
              <font size="3">Ankan Kumar Bhunia</font>, <font size="3">Aishik Konwer</font>, <strong><font size="3"> Ayan Kumar Bhunia </font></strong>,
              <font size="3">Abir Bhowmick</font>, <font size="3">Partha Pratim Roy</font>, <font size="3">Umapada Pal</font>
              <br><font size="3">
              <em>Pattern Recognition (<strong><font size="3.5">PR</font></strong>)</em>, 2019
              </font>
              <br>
              <p></p>
              <a href="javascript:void(0);" onclick="myFunction('PR19_1_abs')"><font size="3">Abstract</font></a> /
              <a href="https://github.com/ankanbhunia/AttenScriptNetPR" target="_blank"><font size="3">Code</font></a> /
              <a href="https://arxiv.org/abs/1801.00470" target="_blank" target="_blank"><font size="3">arXiv</font></a> /
              <a href="javascript:void(0);" onclick="myFunction('PR19_1_bib')"><font size="3">BibTex</font></a>
              <p></p>
              <div id="PR19_1_abs" style="display:none; text-align:justify;min-width:350px;"><font size="3">
                <em>
                Script identification plays a significant role in analysing documents and videos. In this paper, we focus on the problem of script identification in scene text images and video scripts. Because of low image quality, complex background and similar layout of characters shared by some scripts like Greek, Latin, etc., text recognition in those cases become challenging. In this paper, we propose a novel method that involves extraction of local and global features using CNN-LSTM framework and weighting them dynamically for script identification. First, we convert the images into patches and feed them into a CNN-LSTM framework. Attention-based patch weights are calculated applying softmax layer after LSTM. Next, we do patch-wise multiplication of these weights with corresponding CNN to yield local features. Global features are also extracted from last cell state of LSTM. We employ a fusion technique which dynamically weights the local and global features for an individual patch. Experiments have been done in four public script identification datasets: SIW-13, CVSI2015, ICDAR-17 and MLe2e. The proposed framework achieves superior results in comparison to conventional methods.
                </em>
              </font></div>
              <div id="PR19_1_bib" style="font-family:Courier;display:none;min-width:350px;"><font size="2">
                <br>
                @article{bhunia2019script,<br>
                  title={Script identification in natural scene image and video frames using an attention based convolutional-LSTM network}, <br>
                  author={Bhunia, Ankan Kumar and Konwer, Aishik and Bhunia, Ayan Kumar and Bhowmick, Abir and Roy, Partha P and Pal, Umapada},
                  journal={Pattern Recognition},
                  volume={85},<br>
                  pages={172--184},<br>
                  year={2019},<br>
                  publisher={Elsevier}
                }
              </font></div>
              </td>
          </tr>
        </tbody></table>
<!--        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>-->
<!--            <tr>-->
<!--            <td style="padding:0px;width:100%;vertical-align:middle;text-align:justify">-->
<!--               <p>-->
<!--              </p>-->
<!--              <heading><font size="5">Affiliations Till Now</font></heading>-->
<!--            </td>-->
<!--          </tr>-->
<!--        </tbody></table>-->





      <table align=center width=600px>
      <tr><td align=center width=600px>
          <center><img src = "./images/logo.png" height="75x"></img><br></center>

      </td></tr>
      </table>

        <!-- Footer - Template Credits -->
        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
          <tr>
            <td style="padding:0px">
              <br>
              <p style="text-align:center;font-size:small;">
                Template credits :
                <a href="https://jonbarron.info/" target="_blank">Dr. Jon Barron</a>
              </p>
            </td>
          </tr>
        </tbody></table>

      </td>
    </tr>
  </table>

        <table align=center width=300px>
      <tr><td align=center width=300px>
          <script type="text/javascript" id="clustrmaps" src="//clustrmaps.com/map_v2.js?d=3ggBviUhnXIQYDHmIz11N6EfhXliwaA0PSN5rvVKWPs&cl=ffffff&w=a"></script>
      </td></tr>
      </table>

<!--        <table align=center width=300px>-->
<!--      <tr><td align=center width=300px>-->
<!--          <script type="application/javascript">-->
<!--    (function(b,o,n,g,s,r,c){if(b[s])return;b[s]={};b[s].scriptToken="XzEwMjgyODE3Mjg";b[s].callsQueue=[];b[s].api=function(){b[s].callsQueue.push(arguments);};r=o.createElement(n);c=o.getElementsByTagName(n)[0];r.async=1;r.src=g;r.id=s+n;c.parentNode.insertBefore(r,c);})(window,document,"script","https://cdn.oribi.io/XzEwMjgyODE3Mjg/oribi.js","ORIBI");-->
<!--    </script>-->
<!--      </td></tr>-->
<!--      </table>-->

</body>

</html>
