<!DOCTYPE HTML>
<html lang="en"><head><meta http-equiv="Content-Type" content="text/html; charset=UTF-8">
  
  <!-- Global site tag (gtag.js) - Google Analytics -->
<script async src="https://www.googletagmanager.com/gtag/js?id=UA-177858501-1"></script>
<script>
  window.dataLayer = window.dataLayer || [];
  function gtag(){dataLayer.push(arguments);}
  gtag('js', new Date());

  gtag('config', 'UA-177858501-1');
</script>
  
  <title>Weijia Wu</title>
  
  <meta name="author" content="Weijia Wu">
  <meta name="viewport" content="width=device-width, initial-scale=1">

  <link rel="stylesheet" type="text/css" href="style.css">
  <link rel="stylesheet" type="text/css" href="stylesheet.css">
  <link rel="icon" type="image/png" href="images/seal_icon.png">
</head>



<body>
  <table style="width:100%;max-width:1000px;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
    <tr style="padding:0px">
      <td style="padding:0px">
        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
          <tr style="padding:0px">
            <td style="padding:2.5%;width:63%;vertical-align:middle">
              <p style="text-align:center">
                <name>Weijia Wu</name>
              </p>

              <p style="text-align:justify"><font size="3">
                  Weijia Wu is currently a research fellow at (<a href="https://sites.google.com/view/showlab" target="_blank" target="_blank"><font size="3">Show Lab</font></a>), National University of Singapore, working with Prof. Mike Z. Shou. I received my PhD from Zhejiang University. Additionally, I am also a popper; I have been dancing popping for five years. In my spare time, I also enjoy playing basketball and swimming.
                </font></p>
   

<!--              Publications (July 2021):-->
<!--               1x<b>NeurIPS</b>, 1x<b>ACCV</b> ， 1x<b>ICIP</b>.-->
			
			
              <p style="text-align:center">
				<a href="weijiawu96@gmail.com" target="_blank"><font size="3">Email </font></a> &nbsp/&nbsp
        <a href="https://scholar.google.com/citations?user=NgjTRe4AAAAJ&hl=en" target="_blank"><font size="3">Google Scholar </font></a> &nbsp/&nbsp
        <a href="https://github.com/weijiawu" target="_blank"><font size="3"> GitHub </font></a> &nbsp/&nbsp
				<a href="https://twitter.com/weijiawu7" target="_blank"><font size="3"> Twitter </font></a> &nbsp/&nbsp
        <a href="https://www.linkedin.com/in/weijia-wu-07a852280/" target="_blank"><font size="3"> Linkedin </font></a>
              </p>
              </td>

            <td style="padding:2.5%;width:40%;max-width:40%">
              <a href="images/weijia11.jpg"><img style="width:50%;max-width:50%" alt="profile photo" src="images/weijia11.jpg" class="hoverZoomLink"></a>

            </td>
          </tr>



        </tbody></table>
        


        <table
            style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;">
            <tbody>
              <tr>
                <td style="padding:20px;width:100%;vertical-align:middle;text-align:justify">
                  <heading>
                    <font size="5">Research Interests</font>
                  </heading>
                  <p>
                    <font size="3">
                      <p>
<!--                      My research focus is broadly centered around scene text detection and recognition, video-and-language tasks(text-based video retrieval, video text tracking and spotting,-->
<!--					  text-based visual question answering).-->
                      
                      My current research interests primarily focus on AI research for camera-based video production, including Video Generation/editing, controllable video generation, and long video generation.

                  <p>
                       <a class="button"   style="color:#FFA500"><strong style="font-size:12px">Notes:</strong></a>
                       <em style="font-size:16px;">
                        Any form of talks and collaboration (job opportunities) is welcomed. Feel free to contact me by Email(weijiawu96@gmail.com). 
                        <!-- Thank you. -->
                           <!-- If there are any potential work opportunities in academia or industry, please feel free to contact me by Email(weijiawu96@gmail.com). Thank you. -->
<!--                          <em style="font-size:16px;"> If you are interested in some potential research collaboration, feel free to contact me by Email(weijiawu@zju.edu.cn)
                            . Most importantly, I would
                            be happy to collaborate with some really self-motivated and enthusiastic undergraduate or post-graduate
                            students who have intention to pursue MS/Ph.D. in future.-->
                  </p>

                  </font>
                  </p>
                </td>
              </tr>
            </tbody>

            
            
            <table
                style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;">
                <tbody>
                  <tr>
                    <td style="padding:20px;width:100%;vertical-align:middle;text-align:justify">
                      <p>
                        <font size="3">
                          <p>
                            <li><sup></strong></font></sup><font size=3.5"><b>Video Text Detection/Spotting</b>: <a href="https://arxiv.org/abs/2203.10539" target="_blank" target="_blank"><font size="3">TransDETR</font></a> (IJCV 2024), <a href="https://link.springer.com/chapter/10.1007/978-3-031-41679-8_23" target="_blank" target="_blank"><font size="3">DSText</font></a> (ICDAR 2023 & PR 2024), <a href="https://arxiv.org/abs/2112.04888" target="_blank" target="_blank"><font size="3">BOVText</font></a> (NeurIPS 2021, Dataset Track)</li>
                            <li><sup></strong></font></sup><font size=3.5"><b>Video Retrieval</b>: <a href="https://www.sciencedirect.com/science/article/abs/pii/S0031320324005697" target="_blank" target="_blank"><font size="3">TextVR</font></a> (PR 2023)
                            <li><sup></strong></font></sup><font size=3.5"><b>Synthetic Data for Perception Tasks</b>: <a href="https://openaccess.thecvf.com/content/ICCV2023/html/Wu_DiffuMask_Synthesizing_Images_with_Pixel-level_Annotations_for_Semantic_Segmentation_Using_ICCV_2023_paper.html" target="_blank" target="_blank"><font size="3">DiffuMask</font></a> (ICCV 2023), <a href="https://proceedings.neurips.cc/paper_files/paper/2023/hash/ab6e7ad2354f350b451b5a8e14d04f51-Abstract-Conference.html" target="_blank" target="_blank"><font size="3">DatasetDM</font></a> (NeurIPS 2023)</li>
                            <li><sup></strong></font></sup><font size=3.5"><b>Image Generation</b>: <a href="https://arxiv.org/abs/2311.14284" target="_blank" target="_blank"><font size="3">ParaDiffusion</font></a> (Arxiv, Nov., 2023)
                            <li><sup></strong></font></sup><font size=3.5"><b>Video Generation</b>: <a href="https://arxiv.org/abs/2403.07420" target="_blank" target="_blank"><font size="3">DragAnything</font></a> (ECCV 2024), <a href="https://github.com/showlab/Awesome-Video-Diffusion" target="_blank" target="_blank"><font size="3">Awesome-Video-Diffusion</font></a> (3.4k stars), <a href="https://weijiawu.github.io/MovieBench/" target="_blank" target="_blank"><font size="3">MovieBench</font></a> (Arxiv, Nov., 2024)
                            <li><sup></strong></font></sup><font size=3.5"><b>Unified Multimodal Models</b>: <a href="https://github.com/showlab/Awesome-Unified-Multimodal-Models" target="_blank" target="_blank"><font size="3">Awesome Unified Multimodal Models</font></a> (220 stars), <a href="https://d2jud02ci9yv69.cloudfront.net/2025-04-28-unified-models-47/blog/unified-models/" target="_blank" target="_blank"><font size="3">Blog--Towards Unified Multimodal Models:Trends and Insights</font></a> 
                      </p>

                      </font>
                      </p>
                    </td>
                  </tr>
                </tbody>
                
<!--            style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;">-->
            <tbody>
              <tr>
                <td style="padding:20px;width:100%;vertical-align:middle;text-align:justify">
                  <heading>
                      <font size="5">Recent Updates </font>
                  </heading>
                  <p>
                    <font size="3">
                            <li><sup></strong></font></sup><font size=3.5">[Sep. 2024]:</font></b> <i><b>One</b></i> paper (<a href="https://openreview.net/pdf?id=5t4ZAkPiJs" target="_blank" target="_blank"><font size="3">ZipCache</font></a>) got accepted in <b>NeurIPS</b> 2024!</li>
                            <li><sup></strong></font></sup><font size=3.5">[Jul. 2024]:</font></b> <i><b>One</b></i> paper (<a href="https://www.sciencedirect.com/science/article/abs/pii/S0031320324005697?CMX_ID=&SIS_ID=&dgcid=STMJ_219742_AUTH_SERV_PA&utm_acid=259162507&utm_campaign=STMJ_219742_AUTH_SERV_PA&utm_in=DM495864&utm_medium=email&utm_source=AC_" target="_blank" target="_blank"><font size="3">TextVR</font></a>) got accepted in <b>Pattern Recognition</b> 2024!</li>
                            <li><sup></strong></font></sup><font size=3.5">[Jul. 2024]:</font></b> <i><b>Three</b></i> papers (<a href="https://arxiv.org/abs/2403.07420" target="_blank" target="_blank"><font size="3">DragAnything</font></a>, <a href="https://arxiv.org/abs/2310.08465" target="_blank" target="_blank"><font size="3">MotionDirector (Oral)</font></a>, <a href="https://arxiv.org/abs/2401.17910" target="_blank" target="_blank"><font size="3">ControlCap</font></a>) got accepted in <b>ECCV</b> 2024!</li>
                            <li><sup></strong></font></sup><font size=3.5">[Mar. 2024]:</font></b> <i><b>One</b></i> paper(<a href="https://arxiv.org/abs/2203.10539" target="_blank" target="_blank"><font size="3">TransDETR</font></a>) got accepted in <b>IJCV</b>!</li>
                            <li><sup></strong></font></sup><font size=3.5">[Feb. 2024]:</font></b> <i><b>One</b></i> paper (<a href="https://openaccess.thecvf.com/content/CVPR2024/html/Fan_DiverGen_Improving_Instance_Segmentation_by_Learning_Wider_Data_Distribution_with_CVPR_2024_paper.html" target="_blank" target="_blank"><font size="3">DiverGen</font></a>) got accepted in <b>CVPR</b> 2024!</li>
                            <li><sup></strong></font></sup><font size=3.5">[Jan. 2024]:</font></b> Got my Ph.D. degree from Zhejiang University!</li>
                            <li><sup></strong></font></sup><font size=3.5">[Jan. 2024]:</font></b> <i><b>One</b></i> paper (<a href="https://arxiv.org/pdf/2305.10657.pdf" target="_blank" target="_blank"><font size="3">EfficientDM</font></a>) got accepted in <b>ICLR</b> 2024 (spotlight)!</li>
                            <li><sup></strong></font></sup><font size=3.5">[Nov. 2023]:</font></b> <i><b>One</b></i> paper (<a href="https://www.sciencedirect.com/science/article/abs/pii/S0031320323008749" target="_blank" target="_blank"><font size="3">DSText V2</font></a>) got accepted in <b>Pattern Recognition</b> 2024!</li>
                            <li><sup></strong></font></sup><font size=3.5">[Nov. 2023]:</font></b> <i><b>One</b></i> paper (<a href="https://arxiv.org/abs/2311.17450" target="_blank" target="_blank"><font size="3">CisDQ</font></a>) got accepted in <b>IEEE TCSVT</b> 2023!</li>
                            <li><sup></strong></font></sup><font size=3.5">[Sep 2023]:</font></b> <i><b>Three</b></i> papers (<a href="https://arxiv.org/abs/2308.06160" target="_blank" target="_blank"><font size="3">DatasetDM</font></a>, <a href="https://arxiv.org/abs/2305.18292" target="_blank" target="_blank"><font size="3">Mix-of-Show</font></a>, <a href="https://proceedings.neurips.cc/paper_files/paper/2023/hash/2aab8a76c7e761b66eccaca0927787de-Abstract-Conference.html" target="_blank" target="_blank"><font size="3">PTQD</font></a>) got accepted in <b>NeurIPS</b> 2023!</li>
                            <li><sup></strong></font></sup><font size=3.5">[July 2023]:</font></b> <i><b>Three</b></i> papers (<a href="https://arxiv.org/abs/2303.11681" target="_blank" target="_blank"><font size="3">DiffuMask</font></a>, <a href="https://openaccess.thecvf.com/content/ICCV2023/html/Zhao_Generative_Prompt_Model_for_Weakly_Supervised_Object_Localization_ICCV_2023_paper.html" target="_blank" target="_blank"><font size="3">GenPromp</font></a>, <a href="https://openaccess.thecvf.com/content/ICCV2023/html/He_BiViT_Extremely_Compressed_Binary_Vision_Transformers_ICCV_2023_paper.html" target="_blank" target="_blank"><font size="3">BiViT</font></a>) got accepted in <b>ICCV</b> 2023!</li>
                            <li><sup></strong></font></sup><font size=3.5">[Dec. 2022]:We organize <a href="https://sites.google.com/view/loveucvpr23/home" target="_blank" target="_blank"><font size="3">LOng-form VidEo Understanding and Generation Workshop & International Challenge @ CVPR'23</font></a>!</li>
                            <li><sup></strong></font></sup><font size=3.5">[Dec. 2022]:We organize <a href="https://rrc.cvc.uab.es/?ch=22&com=introduction" target="_blank" target="_blank"><font size="3">ICDAR2023 Video Text Reading Competition for Dense and Small Text</font></a>!</li>
							<li><sup></strong></font></sup><font size=3.5">[June 2022]:</font></b> <i><b>One</b></i> paper got accepted in <b>ICIP</b> 2022!</li>
							<li><sup></strong></font></sup><font size=3.5">[March 2022]:</font></b><i><b></b></i>Serve as a reviewer for ICML2022.</li>
                        	<li><sup></strong></font></sup><font size=3.5">[July 2021]:</font></b> <i><b>One</b></i> paper got accepted in <b>NeurIPS</b> 2021!!</li>
							<li><sup></strong></font></sup><font size=3.5">[June 2021]:</font></b><i><b></b></i>Serve as a reviewer for NeurIPS2021.</li>



                           

<!--<a href="http://personal.ee.surrey.ac.uk/Personal/Y.Song/" target="_blank"><font size="3">-->
<!--                Dr. Yi-Zhe Song</font></a>-->
                  </p>

                  </font>
                  </p>
                </td>
              </tr>
            </tbody>

			
			<tbody>
              <tr>
                <td style="padding:20px;width:100%;vertical-align:middle;text-align:justify">
                  <heading>
                      <font size="5">Experience </font>
                  </heading>
                  <p>
                    <font size="3">
                            
							<li><sup></strong></font></sup><font size=3.5">[January 2021 - January 2022]:</font></b><i><b></b></i>Research Intern at MMU, KuaiShou, led by <a href="https://scholar.google.com/citations?user=4nL1cDEAAAAJ&hl=en" target="_blank" target="_blank"><font size="3">Debing Zhang</font></a>
							<li><sup></strong></font></sup><font size=3.5">[January 2022 - August 2023]:</font></b><i><b></b></i>Research Intern at MMU, KuaiShou, led by <a href="https://scholar.google.com/citations?user=cPyOunQAAAAJ" target="_blank" target="_blank"><font size="3">Jiahong Li</font></a>
                            <li><sup></strong></font></sup><font size=3.5">[August 2022 - August 2023]:</font></b><i><b></b></i>Visting PhD student at <a href="https://sites.google.com/view/showlab" target="_blank" target="_blank"><font size="3">Show lab</font></a>, NUS, led by <a href="https://scholar.google.com/citations?user=h1-3lSoAAAAJ&hl=zh-CN" target="_blank" target="_blank"><font size="3">Asst Prof. Mike Shou</font></a>
                            <li><sup></strong></font></sup><font size=3.5">[Dec. 2023 - Present]:</font></b><i><b></b></i>Research Fellow at <a href="https://sites.google.com/view/showlab" target="_blank" target="_blank"><font size="3">Show lab</font></a>, NUS, work with <a href="https://scholar.google.com/citations?user=h1-3lSoAAAAJ&hl=zh-CN" target="_blank" target="_blank"><font size="3">Asst Prof. Mike Shou</font></a>

                  </p>

                  </font>
                  </p>
                </td>
              </tr>
            </tbody>
			
		
          </table>
		  


<!--&lt;!&ndash; Updates on Recent Activities  &ndash;&gt;-->
<!--          <table-->
<!--            style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;">-->
<!--            <tbody>-->
<!--              <tr>-->
<!--                <td-->
<!--                  style="padding:20px;padding-top:5px;padding-bottom:5px;width:100%;vertical-align:middle;text-align:justify">-->
<!--                  <heading>-->
<!--                    <font size="5">Updates</font>-->
<!--                  </heading>-->
<!--                  <p>-->
<!--                    <font size="3">-->
<!--                      &lt;!&ndash; <a class="button" href="#" style="color:#FFA500"><strong style="font-size:12px">New</strong></a> &ndash;&gt;-->
<!--                      &lt;!&ndash; <button type="submit" class="button">Click me!</button> &ndash;&gt;-->
<!--                      <ul style="padding-inline-start:0px;list-style-type:none;">-->
<!--                        &lt;!&ndash;                         <li> <a class="button" href="#" style="color:#FFA500"><strong style="font-size:12px">New</strong></a><em style="font-size:16px;"> 1 paper on Future Frame Depth Prediction communicated to <strong style="font-size:16px">ECCV 2020</strong></em></li> &ndash;&gt;-->
<!--                        <p style="margin-block-start:0px;margin-block-end:3px"></p>-->
<!--                        <li> <a class="button" href="#" style="color:#FFA500"><strong-->
<!--                              style="font-size:12px">New</strong></a> <strong-->
<!--                            style="font-size:15px;padding-left:5px;">03/2021 : </strong>-->
<!--                          <em style="font-size:16px;"> My 1st PhD work has been communicated to a top-tier Computer Vision conference.-->

<!--                        <p style="margin-block-start:0px;margin-block-end:3px"></p>-->
<!--                        <li> <a class="button" href="#" style="color:#FFA500"><strong-->
<!--                              style="font-size:12px">New</strong></a> <strong-->
<!--                            style="font-size:15px;padding-left:5px;">01/2021 : </strong>-->
<!--                          <em style="font-size:16px;"> I am serving as a reviewer for <strong><a href="https://www.springer.com/journal/42979?gclid=Cj0KCQjwgtWDBhDZARIsADEKwgNEqQSbM5pYHdDB-Af_i4KITk24r5r8GgbSfwhvxYlRTaCtx8QLKIYaAswyEALw_wcB"-->
<!--                                style="font-size:16px" >Springer Computer Science </a> </strong> journal.-->

<!--                        <p style="margin-block-start:0px;margin-block-end:3px"></p>-->
<!--                        <li> <a class="button" href="#" style="color:#FFA500"><strong-->
<!--                              style="font-size:12px">New</strong></a> <strong-->
<!--                            style="font-size:15px;padding-left:5px;">09/2020 : </strong>-->
<!--                          <em style="font-size:16px;"> I have moved to South-West London to join<strong style="font-size:16px"> ReID-->
<!--                              Lab,-->
<!--                              University of Surrey</strong> as a PhD student.-->



<!--                        <p style="margin-block-start:0px;margin-block-end:3px"></p>-->
<!--                        <li> <a class="button" href="#" style="color:#FFA500"><strong-->
<!--                              style="font-size:12px">New</strong></a> <strong-->
<!--                            style="font-size:15px;padding-left:5px;">05/2020 : </strong>-->
<!--                          <em style="font-size:16px;"> <strong>1</strong> paper communicated to <strong-->
<!--                              style="font-size:16px"> <a-->
<!--                                href="https://ieeexplore.ieee.org/xpl/RecentIssue.jsp?punumber=76"-->
<!--                                style="font-size:16px">IEEE Transactions on Circuits and Systems for Video-->
<!--                                Technology</a> </strong></em></li>-->

<!--                        <p style="margin-block-start:0px;margin-block-end:3px"></p>-->
<!--                        <li> <a class="button" href="#" style="color:#FFA500"><strong-->
<!--                              style="font-size:12px">New</strong></a> <strong-->
<!--                            style="font-size:15px;padding-left:5px;">05/2020 : </strong>-->
<!--                          <em style="font-size:16px;"> <strong>1</strong> paper got accepted in <strong-->
<!--                              style="font-size:16px"> <a href="https://www.journals.elsevier.com/pattern-recognition"-->
<!--                                style="font-size:16px">Pattern Recognition, Elsevier</a> </strong></em></li>-->


<!--                        <p style="margin-block-start:0px;margin-block-end:3px"></p>-->
<!--                        <li style="padding-left:40px;">-->
<!--                          &lt;!&ndash; <a class="button" href="#" style="color:#FFA500"><strong-->
<!--                              style="font-size:12px">New</strong></a>  &ndash;&gt;-->
<!--                          <strong style="font-size:15px;padding-left:5px;">11/2019 : </strong>-->
<!--                          <em style="font-size:16px;"> Our paper titled-->
<!--                            "What's There in The Dark" has been selected in <strong style="font-size:16px"><a-->
<!--                                style="font-size:16px" href="http://2019.ieeeicip.org/?action=page3&id=9">TOP 10%-->
<!--                                Papers</a></strong> in <strong style="font-size:16px">ICIP19</strong></em></li>-->

<!--                        <p style="margin-block-start:0px;margin-block-end:3px"></p>-->
<!--                        <li style="padding-left:45px;"> <em style="font-size:16px;"></em> <strong-->
<!--                            style="font-size:15px">10/2019 : </strong> </em> <strong>1</strong> paper on <strong-->
<!--                            style="font-size:16px">Light-weight Saliency Detection</strong> is uploaded on Arxiv</em>-->
<!--                        </li>-->
<!--                      </ul>-->
<!--                    </font>-->
<!--                  </p>-->
<!--                </td>-->
<!--              </tr>-->
<!--            </tbody>-->
<!--          </table>-->




<!--        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>-->
<!--            <tr>-->
<!--            <td style="padding:0px;width:100%;vertical-align:middle;text-align:justify">-->
<!--              <heading><font size="5">Research Interests</font></heading>-->
<!--              <p><font size="3">-->
<!--                I am broadly interested in the field of Computer Vision and Deep Learning. Particularly, I like to think upon <b>Visual Scene Understanding (VSU)</b> from images and videos, effective methods of <b>Domain Adaptation & Transfer Learning</b> for VSU, building <b>systems that learn with minimal or no supervision</b> and <b>systems that generalize well</b> in real and diverse scenarios.-->
<!--                I am also open to any topic that would be interesting or fun to explore and pursue.-->
<!--              </font></p>-->
<!--            </td>-->
<!--          </tr>-->
<!--        </tbody></table>-->

        <!-- PUBLICATIONS -->
        <table style="width:95%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
            <tr>
            <td style="padding:0px;width:100%;vertical-align:middle;text-align:justify">
               <p>
              </p>
              <heading><font size="5">Selected Publications</font></heading>
            </td>
          </tr>
        </tbody></table>

        <script>
          function myFunction(pub_name) {
              var x = document.getElementById(pub_name);
              if (x.style.display === 'none') {
                  x.style.display = 'block';
              } else {
                  x.style.display = 'none';
              }
        }
        </script>
        
        
        <table style="width:85%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
            <tr>
            <td style="padding:0px;width:100%;vertical-align:middle;text-align:justify">
               <p>
              </p>
              <heading><font size="5">2024</font></heading>
            </td>
          </tr>
        </tbody></table>
        
        <table style="width:80%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
        
        <!-- DragAnything -->
          <tr onmouseout="DragAnything_stop()" onmouseover="DragAnything_start()">
            <td style="padding:20px;width:25%;vertical-align:middle">
              <div class="one">
                <div class="two" id='DragAnything'><img src='images/Draganything.jpg'></div>
                <img src='images/Draganything.jpg'>
              </div>
              <script type="text/javascript">
                function DragAnything_start() {
                  document.getElementById('DragAnything_PTDQ').style.opacity = "1";
                }

                function DragAnything_stop() {
                  document.getElementById('DragAnything_PTDQ').style.opacity = "0";
                }
                  DragAnything_stop()
              </script>
            </td>
            <td style="padding:10px;width:75%;vertical-align:middle">
              <a href="https://arxiv.org/pdf/2403.07420" target="_blank">
                <papertitle><font size="3">DragAnything: Motion Control for Anything using Entity Representation</font></papertitle>
              </a>
              <br><br>
              <strong><font size="3"> Weijia Wu</font></strong>, <font size="3"> Zhuang Li</font>, <font size="3"> Yuchao Gu</font>,<font size="3"> Rui Zhao</font>,<font size="3"> Yefei He</font>,<font size="3"> David Junhao Zhang</font>,<font size="3"> Mike Zheng Shou</font>,<font size="3"> Yan Li</font>,<font size="3"> Tingting Gao</font>,<font size="3"> Di Zhang</font>.
              <br><font size="3">
                    <em>The 18th European Conference on Computer Vision(<strong><font size="3.5">ECCV 2024</font></strong>)</em>,
              </font>
              <br>
              <p></p>
              <a href="javascript:void(0);" onclick="myFunction('DragAnything_abs')"><font size="3">Abstract</font></a> /
             <a href="https://arxiv.org/pdf/2403.07420" target="_blank" target="_blank"><font size="3">arXiv</font></a> /
              <a href="javascript:void(0);" onclick="myFunction('DragAnything_bib')"><font size="3">BibTex</font></a> /
              <a href="https://github.com/showlab/DragAnything" target="_blank" target="_blank"><font size="3">GitHub</font></a> /
              <a href="https://weijiawu.github.io/draganything_page/" target="_blank" target="_blank"><font size="3">Project Page</font></a> /
              <a href="https://x.com/_akhaliq/status/1767737635064127749" target="_blank" target="_blank"><font size="3">Twitter(X)</font></a> /
              <p></p>
              <div id="DragAnything_abs" style="display:none; text-align:justify;min-width:350px;"><font size="3">
                <em> We introduce DragAnything, which utilizes a entity representation to achieve motion control for any object in controllable video
                    generation. Comparison to existing motion control methods, DragAnything offers several advantages. Firstly, trajectory-based is more userfriendly for interaction, when acquiring other guidance signals (e.g.,
                    masks, depth maps) is labor-intensive. Users only need to draw a line (trajectory) during interaction. Secondly, our entity representation serves as
                    an open-domain embedding capable of representing any object, enabling
                    the control of motion for diverse entities, including background. Lastly,
                    our entity representation allows simultaneous and distinct motion control for multiple objects. Extensive experiments demonstrate that our
                    DragAnything achieves state-of-the-art performance for FVD, FID, and
                    User Study, particularly in terms of object motion control, where our
                    method surpasses the previous methods (e.g., DragNUWA) by 26% in
                    human voting.
              </font></div>
              <div id="DragAnything_bib" style="font-family:Courier;display:none;min-width:350px;"><font size="2">
                <br>
                @article{wu2024draganything,
                  title={DragAnything: Motion Control for Anything using Entity Representation},
                  author={Wu, Wejia and Li, Zhuang and Gu, Yuchao and Zhao, Rui and He, Yefei and Zhang, David Junhao and Shou, Mike Zheng and Li, Yan and Gao, Tingting and Zhang, Di},
                  journal={The 18th European Conference on Computer Vision (ECCV 2024)},
                  year={2024}<br>
                }
              </font></div>
              </td>
          </tr>
          
          
        <!-- EfficientDM -->
          <tr onmouseout="EfficientDM_stop()" onmouseover="EfficientDM_start()">
            <td style="padding:20px;width:25%;vertical-align:middle">
              <div class="one">
                <div class="two" id='EfficientDM'><img src='images/1705411673940.jpg'></div>
                <img src='images/1705411673940.jpg'>
              </div>
              <script type="text/javascript">
                function bivit_start() {
                  document.getElementById('EfficientDM_PTDQ').style.opacity = "1";
                }

                function bivit_stop() {
                  document.getElementById('EfficientDM_PTDQ').style.opacity = "0";
                }
                  EfficientDM_stop()
              </script>
            </td>
            <td style="padding:10px;width:75%;vertical-align:middle">
              <a href="https://arxiv.org/pdf/2305.10657.pdf" target="_blank">
                <papertitle><font size="3">EfficientDM: Efficient Quantization-Aware Fine-Tuning of Low-Bit Diffusion Models</font></papertitle>
              </a>
              <br><br>
              <font size="3">Yefei He</font>, <font size="3"> Jing Liu</font>, <strong><font size="3"> Weijia Wu</font></strong>,<font size="3"> Hong Zhou</font>,<font size="3"> Bohan Zhuang</font>.
              <br><font size="3">
                    <em>The Twelfth International Conference on Learning Representations(<strong><font size="3.5">ICLR 2024 spotlight</font></strong>)</em>,
              </font>
              <br>
              <p></p>
              <a href="javascript:void(0);" onclick="myFunction('EfficientDM_abs')"><font size="3">Abstract</font></a> /
             <a href="https://arxiv.org/pdf/2310.03270.pdf" target="_blank" target="_blank"><font size="3">arXiv</font></a> /
              <a href="javascript:void(0);" onclick="myFunction('EfficientDM_bib')"><font size="3">BibTex</font></a> /
              <p></p>
              <div id="EfficientDM_abs" style="display:none; text-align:justify;min-width:350px;"><font size="3">
                <em> Diffusion models have demonstrated remarkable capabilities in image synthesis and related generative tasks. Nevertheless, their practicality for low-latency
                    real-world applications is constrained by substantial computational costs and latency issues. Quantization is a dominant way to compress and accelerate diffusion
                    models, where post-training quantization (PTQ) and quantization-aware training
                    (QAT) are two main approaches, each bearing its own properties. While PTQ exhibits efficiency in terms of both time and data usage, it may lead to diminished
                    performance in low bit-width settings. On the other hand, QAT can help alleviate performance degradation but comes with substantial demands on computational and data resources. To capitalize on the advantages while avoiding their respective drawbacks, we introduce a data-free, quantization-aware and parameterefficient fine-tuning framework for low-bit diffusion models, dubbed EfficientDM,
                    to achieve QAT-level performance with PTQ-like efficiency. Specifically, we propose a quantization-aware variant of the low-rank adapter (QALoRA) that can
                    be merged with model weights and jointly quantized to low bit-width. The finetuning process distills the denoising capabilities of the full-precision model into
                    its quantized counterpart, eliminating the requirement for training data. To further
                    enhance performance, we introduce scale-aware optimization to address ineffective learning of QALoRA due to variations in weight quantization scales across
                    different layers. We also employ temporal learned step-size quantization to handle
                    notable variations in activation distributions across denoising steps. Extensive experimental results demonstrate that our method significantly outperforms previous
                    PTQ-based diffusion models while maintaining similar time and data efficiency.
                    Specifically, there is only a marginal 0.05 sFID increase when quantizing both
                    weights and activations of LDM-4 to 4-bit on ImageNet 256 × 256. Compared
                    to QAT-based methods, our EfficientDM also boasts a 16.2× faster quantization
                    speed with comparable generation quality, rendering it a compelling choice for
                    practical applications.
              </font></div>
              <div id="EfficientDM_bib" style="font-family:Courier;display:none;min-width:350px;"><font size="2">
                <br>
                @article{he2023efficientdm,
                  title={EfficientDM: Efficient Quantization-Aware Fine-Tuning of Low-Bit Diffusion Models},
                  author={He, Yefei and Liu, Jing and Wu, Weijia and Zhou, Hong and Zhuang, Bohan},
                  journal={The Twelfth International Conference on Learning Representations (ICLR 2024)},
                  year={2024}<br>
                }
              </font></div>
              </td>
          </tr>
          
          
        <table style="width:85%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
            <tr>
            <td style="padding:0px;width:100%;vertical-align:middle;text-align:justify">
               <p>
              </p>
              <heading><font size="5">2023</font></heading>
            </td>
          </tr>
        </tbody></table>

        <table style="width:80%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
        
        <!-- PTDQ -->
          <tr onmouseout="PTDQ_stop()" onmouseover="PTDQ_start()">
            <td style="padding:20px;width:25%;vertical-align:middle">
              <div class="one">
                <div class="two" id='PTDQ'><img src='images/PTDQ.jpg'></div>
                <img src='images/PTDQ.jpg'>
              </div>
              <script type="text/javascript">
                function bivit_start() {
                  document.getElementById('Neurips2023_PTDQ').style.opacity = "1";
                }

                function bivit_stop() {
                  document.getElementById('Neurips2023_PTDQ').style.opacity = "0";
                }
                            PTDQ_stop()
              </script>
            </td>
            <td style="padding:10px;width:75%;vertical-align:middle">
              <a href="https://arxiv.org/pdf/2305.10657.pdf" target="_blank">
                <papertitle><font size="3">PTQD: Accurate Post-Training Quantization for Diffusion Models†</font></papertitle>
              </a>
              <br><br>
              <font size="3">Yefei He</font>, <font size="3">Luping Liu</font>, <font size="3"> Jing Liu</font>, <strong><font size="3"> Weijia Wu</font></strong>,<font size="3"> Hong Zhou</font>,<font size="3"> Bohan Zhuang</font>.
              <br><font size="3">
                    <em>Thirty-seventh Conference on Neural Information Processing Systems(<strong><font size="3.5">NeurIPS 2023</font></strong>)</em>,
              </font>
              <br>
              <p></p>
              <a href="javascript:void(0);" onclick="myFunction('PTQD_abs')"><font size="3">Abstract</font></a> /
             <a href="https://arxiv.org/pdf/2305.10657.pdf" target="_blank" target="_blank"><font size="3">arXiv</font></a> /
              <a href="javascript:void(0);" onclick="myFunction('PTQD_bib')"><font size="3">BibTex</font></a> /
              <p></p>
              <div id="PTQD_abs" style="display:none; text-align:justify;min-width:350px;"><font size="3">
                <em> Diffusion models have recently dominated image synthesis and other related generative tasks. However, the iterative denoising process is expensive in computations at inference time, making diffusion models less practical for low-latency and scalable real-world applications. Post-training quantization of diffusion models can significantly reduce the model size and accelerate the sampling process without requiring any re-training. Nonetheless, applying existing post-training quantization methods directly to low-bit diffusion models can significantly impair the quality of generated samples. Specifically, for each denoising step, quantization noise leads to deviations in the estimated mean and mismatches with the predetermined variance schedule. Moreover, as the sampling process proceeds, the quantization noise may accumulate, resulting in a low signal-to-noise ratio (SNR) in late denoising steps. To address these challenges, we propose a unified formulation for the quantization noise and diffusion perturbed noise in the quantized denoising process. We first disentangle the quantization noise into its correlated and residual uncorrelated parts regarding its full-precision counterpart. The correlated part can be easily corrected by estimating the correlation coefficient. For the uncorrelated part, we calibrate the denoising variance schedule to absorb the excess variance resulting from quantization. Moreover, we propose a mixed-precision scheme to choose the optimal bitwidth for each denoising step, which prefers low bits to accelerate the early denoising steps while high bits maintain the high SNR for the late steps.
              </font></div>
              <div id="PTQD_bib" style="font-family:Courier;display:none;min-width:350px;"><font size="2">
                <br>
                @article{he2023ptqd,
                  title={PTQD: Accurate Post-Training Quantization for Diffusion Models},
                  author={He, Yefei and Liu, Luping and Liu, Jing and Wu, Weijia and Zhou, Hong and Zhuang, Bohan},
                  journal={Proc. Advances In Neural Information Processing Systems (NeurIPS 2023)},
                  year={2023}<br>
                }
              </font></div>
              </td>
          </tr>
                
        <!-- DatasetDM -->
          <tr onmouseout="bivit_stop()" onmouseover="DatasetDM_start()">
            <td style="padding:20px;width:25%;vertical-align:middle">
              <div class="one">
                <div class="two" id='DatasetDM'><img src='images/datasetdm.jpg'></div>
                <img src='images/datasetdm.jpg'>
              </div>
              <script type="text/javascript">
                function bivit_start() {
                  document.getElementById('Neurips2023').style.opacity = "1";
                }

                function bivit_stop() {
                  document.getElementById('Neurips2023').style.opacity = "0";
                }
                  bivit_stop()
              </script>
            </td>
            <td style="padding:10px;width:75%;vertical-align:middle">
              <a href="https://arxiv.org/abs/2308.06160" target="_blank">
                <papertitle><font size="3">DatasetDM: Synthesizing Data with Perception Annotations Using Diffusion Models</font></papertitle>
              </a>
              <br><br>
              <strong><font size="3"> Weijia Wu</font></strong>, <font size="3">Yuzhong Zhao</font>, <font size="3"> Hao Chen</font>, <font size="3"> Yuchao Gu</font>,<font size="3"> Rui Zhao</font>,<font size="3"> Yefei He</font>, <font size="3"> Hong Zhou</font>, <font size="3"> Mike Zheng Shou</font>,  <font size="3">  Chunhua Shen</font>.
              <br><font size="3">
                    <em>Thirty-seventh Conference on Neural Information Processing Systems(<strong><font size="3.5">NeurIPS 2023</font></strong>)</em>,
              </font>
              <br>
              <p></p>
              <a href="javascript:void(0);" onclick="myFunction('datasetdm_abs')"><font size="3">Abstract</font></a> /
             <a href="https://arxiv.org/abs/2308.06160" target="_blank" target="_blank"><font size="3">arXiv</font></a> /
              <a href="javascript:void(0);" onclick="myFunction('datasetdm_bib')"><font size="3">BibTex</font></a> /
             <a href="https://github.com/showlab/DatasetDM" target="_blank" target="_blank"><font size="3">GitHub</font></a> /
             <a href="https://weijiawu.github.io/DatasetDM_page/" target="_blank" target="_blank"><font size="3">Project Page</font></a> /
             <a href="https://mp.weixin.qq.com/s/VAMxr9Ot9jKnGHkBnla-fA" target="_blank" target="_blank"><font size="3">中文公众号报道</font></a> /
              <p></p>
              <div id="datasetdm_abs" style="display:none; text-align:justify;min-width:350px;"><font size="3">
                <em> Current deep networks are very data-hungry and benefit from training on largescale datasets, which are often time-consuming to collect and annotate. By contrast, synthetic data can be generated infinitely using generative models such as DALL-E and diffusion models, with minimal effort and cost. In this paper, we present DatasetDM, a generic dataset generation model that can produce diverse synthetic images and the corresponding high-quality perception annotations (e.g., segmentation masks, and depth). Our method builds upon the pre-trained diffusion model and extends text-guided image synthesis to perception data generation. We show that the rich latent code of the diffusion model can be effectively decoded as accurate perception annotations using a decoder module. Training the decoder only needs less than 1% (around 100 images) manually labeled images, enabling the generation of an infinitely large annotated dataset. Then these synthetic data can be used for training various perception models for downstream tasks. To showcase the power of the proposed approach, we generate datasets with rich dense pixel-wise labels for a wide range of downstream tasks, including semantic segmentation, instance segmentation, and depth estimation. Notably, it achieves 1) state-of-the-art results on semantic segmentation and instance segmentation; 2) significantly more robust on domain generalization than using the real data alone; and state-of-the-art results in zero-shot segmentation setting; and 3) flexibility for efficient application and novel task composition (e.g., image editing).
              </font></div>
              <div id="datasetdm_bib" style="font-family:Courier;display:none;min-width:350px;"><font size="2">
                <br>
                @article{wu2023datasetdm,
                  title={DatasetDM: Synthesizing Data with Perception Annotations Using Diffusion Models},
                  author={Wu, Weijia and Zhao, Yuzhong and Chen, Hao and Gu, Yuchao and Zhao, Rui and He, Yefei and Zhou, Hong and Shou, Mike Zheng and Shen, Chunhua},
                  journal={Proc. Advances In Neural Information Processing Systems (NeurIPS 2023)},
                  year={2023}<br>
                }
              </font></div>
              </td>
          </tr>
                
                
                
        <!-- BiViT -->
          <tr onmouseout="bivit_stop()" onmouseover="bivit_start()">
            <td style="padding:20px;width:25%;vertical-align:middle">
              <div class="one">
                <div class="two" id='bivit'><img src='images/bivit.jpg'></div>
                <img src='images/bivit.jpg'>
              </div>
              <script type="text/javascript">
                function bivit_start() {
                  document.getElementById('ICCV2023').style.opacity = "1";
                }

                function bivit_stop() {
                  document.getElementById('ICCV2023').style.opacity = "0";
                }
                  bivit_stop()
              </script>
            </td>
            <td style="padding:10px;width:75%;vertical-align:middle">
              <a href="https://arxiv.org/abs/2211.07091" target="_blank">
                <papertitle><font size="3">BiViT: Extremely Compressed Binary Vision Transformers</font></papertitle>
              </a>
              <br><br>
              <font size="3">Yefei He</font>,<font size="3">Zhenyu Lou</font>,,<font size="3">Luoming Zhang</font><strong><font size="3"> Weijia Wu</font></strong>,  <font size="3"> Bohan Zhuang</font>, <font size="3"> Hong Zhou</font>.
              <br><font size="3">
                    <em>International Conference on Computer Vision Conference (<strong><font size="3.5">ICCV2023</font></strong>)</em>,
              </font>
              <br>
              <p></p>
              <a href="javascript:void(0);" onclick="myFunction('bivit_abs')"><font size="3">Abstract</font></a> /
             <a href="https://arxiv.org/abs/2211.07091" target="_blank" target="_blank"><font size="3">arXiv</font></a> /
              <a href="javascript:void(0);" onclick="myFunction('bivit_bib')"><font size="3">BibTex</font></a> /
              <p></p>
              <div id="bivit_abs" style="display:none; text-align:justify;min-width:350px;"><font size="3">
                <em> Model binarization can significantly compress model size, reduce energy consumption, and accelerate inference through efficient bit-wise operations. Although binarizing convolutional neural networks have been extensively studied, there is little work on exploring binarization on vision Transformers which underpin most recent breakthroughs in visual recognition. To this end, we propose to solve two fundamental challenges to push the horizon of Binary Vision Transformers (BiViT). First, the traditional binary method does not take the long-tailed distribution of softmax attention into consideration, bringing large binarization errors in the attention module. To solve this, we propose Softmax-aware Binarization, which dynamically adapts to the data distribution and reduces the error caused by binarization. Second, to better exploit the information of the pretrained model and restore accuracy, we propose a Cross-layer Binarization scheme and introduce learnable channel-wise scaling factors for weight binarization. The former decouples the binarization of self-attention and MLP to avoid mutual interference while the latter enhances the representation capacity of binarized models. Overall, our method performs favorably against state-of-the-arts by 19.8% on the TinyImageNet dataset. On ImageNet, BiViT achieves a competitive 70.8% Top-1 accuracy over Swin-T model, outperforming the existing SOTA methods by a clear margin.
              </font></div>
              <div id="bivit_bib" style="font-family:Courier;display:none;min-width:350px;"><font size="2">
                <br>
                @InProceedings{he2022bivit,<br>
                title={Bivit: Extremely compressed binary vision transformer},
                author={He, Yefei and Lou, Zhenyu and Zhang, Luoming and Wu, Weijia and Zhuang, Bohan and Zhou, Hong},
                booktitle = {International Conference on Computer Vision Conference 2023},<br>
                year = {2023}<br>
                }
              </font></div>
              </td>
          </tr>
          
          
        <!-- GPM -->
          <tr onmouseout="GPM_stop()" onmouseover="GPM_start()">
            <td style="padding:20px;width:25%;vertical-align:middle">
              <div class="one">
                <div class="two" id='GPM'><img src='images/GPM.jpg'></div>
                <img src='images/GPM.jpg'>
              </div>
              <script type="text/javascript">
                function GPM_start() {
                  document.getElementById('ICCV2023').style.opacity = "1";
                }

                function GPM_stop() {
                  document.getElementById('ICCV2023').style.opacity = "0";
                }
                  GPM_stop()
              </script>
            </td>
            <td style="padding:10px;width:75%;vertical-align:middle">
              <a href="https://arxiv.org/abs/2307.09756" target="_blank">
                <papertitle><font size="3">Generative Prompt Model for Weakly Supervised Object Localization</font></papertitle>
              </a>
              <br><br>
              <font size="3">Yuzhong Zhao</font>,<font size="3">Qixiang Ye</font>,<strong><font size="3"> Weijia Wu</font></strong>,  <font size="3"> Chunhua Shen</font>, <font size="3"> Fan Wan</font>.
              <br><font size="3">
                    <em>International Conference on Computer Vision Conference (<strong><font size="3.5">ICCV2023</font></strong>)</em>,
              </font>
              <br>
              <p></p>
              <a href="javascript:void(0);" onclick="myFunction('GPM_abs')"><font size="3">Abstract</font></a> /
             <a href="https://arxiv.org/abs/2307.09756" target="_blank" target="_blank"><font size="3">arXiv</font></a> /
              <a href="javascript:void(0);" onclick="myFunction('GPM_bib')"><font size="3">BibTex</font></a> /
              <a href="https://github.com/callsys/GenPromp" target="_blank" target="_blank"><font size="3">GitHub</font></a> /
              <p></p>
              <div id="GPM_abs" style="display:none; text-align:justify;min-width:350px;"><font size="3">
                <em> Weakly supervised object localization (WSOL) remains challenging when learning object localization models from image category labels. Conventional methods that discriminatively train activation models ignore representative yet less discriminative object parts. In this study, we propose a generative prompt model (GenPromp), defining the first generative pipeline to localize less discriminative object parts by formulating WSOL as a conditional image denoising procedure. During training, GenPromp converts image category labels to learnable prompt embeddings which are fed to a generative model to conditionally recover the input image with noise and learn representative embeddings. During inference, enPromp combines the representative embeddings with discriminative embeddings (queried from an off-the-shelf vision-language model) for both representative and discriminative capacity. The combined embeddings are finally used to generate multi-scale high-quality attention maps, which facilitate localizing full object extent. Experiments on CUB-200-2011 and ILSVRC show that GenPromp respectively outperforms the best discriminative models by 5.2% and 5.6% (Top-1 Loc), setting a solid baseline for WSOL with the generative model.
              </font></div>
              <div id="GPM_bib" style="font-family:Courier;display:none;min-width:350px;"><font size="2">
                <br>
                @InProceedings{zhao2023GPM,<br>
                author = {Yuzhong Zhao, Qixiang Ye, Weijia Wu, Chunhua Shen, Fang Wan},<br>
                title = {Generative Prompt Model for Weakly Supervised Object Localization},<br>
                booktitle = {International Conference on Computer Vision Conference 2023},<br>
                year = {2023}<br>
                }
              </font></div>
              </td>
          </tr>
          
          
        <!-- DiffuMask -->
          <tr onmouseout="DiffuMask_stop()" onmouseover="DiffuMask_start()">
            <td style="padding:20px;width:25%;vertical-align:middle">
              <div class="one">
                <div class="two" id='DiffuMask'><img src='images/diffumask.png'></div>
                <img src='images/DiffuMask.jpg'>
              </div>
              <script type="text/javascript">
                function DiffuMask_start() {
                  document.getElementById('neurips2021').style.opacity = "1";
                }

                function DiffuMask_stop() {
                  document.getElementById('neurips2021').style.opacity = "0";
                }
                  DiffuMask_stop()
              </script>
            </td>
            <td style="padding:10px;width:75%;vertical-align:middle">
              <a href="https://arxiv.org/abs/2303.11681" target="_blank">
                <papertitle><font size="3">DiffuMask: Synthesizing Images with Pixel-level Annotations for Semantic Segmentation Using Diffusion Models</font></papertitle>
              </a>
              <br><br>
               <strong><font size="3"> Weijia Wu</font></strong>, <font size="3">Yuzhong Zhao</font>, <font size="3"> Mike Zheng Shou</font>, <font size="3"> Hong Zhou</font>, <font size="3">  Chunhua Shen</font>.
              <br><font size="3">
                    <em>International Conference on Computer Vision Conference (<strong><font size="3.5">ICCV2023</font></strong>)</em>,
              </font>
              <br>
              <p></p>
              <a href="javascript:void(0);" onclick="myFunction('DiffuMask_abs')"><font size="3">Abstract</font></a> /
             <a href="https://arxiv.org/abs/2303.11681" target="_blank" target="_blank"><font size="3">arXiv</font></a> /
              <a href="javascript:void(0);" onclick="myFunction('DiffuMask_bib')"><font size="3">BibTex</font></a> /
              <a href="https://github.com/weijiawu/DiffuMask" target="_blank" target="_blank"><font size="3">GitHub</font></a> /
              <a href="https://weijiawu.github.io/DiffusionMask/" target="_blank" target="_blank"><font size="3">Project Page</font></a> /
              <p></p>
              <div id="DiffuMask_abs" style="display:none; text-align:justify;min-width:350px;"><font size="3">
                <em> Collecting and annotating images with pixel-wise labels is time-consuming and laborious. In contrast, synthetic data can be freely available using a generative model (e.g., DALL-E, Stable Diffusion). In this paper, we show that it is possible to automatically obtain accurate semantic masks of synthetic images generated by the Off-the-shelf Stable Diffusion model, which uses only text-image pairs during training. Our approach, called DiffuMask, exploits the potential of the cross-attention map between text and image, which is natural and seamless to extend the text-driven image synthesis to semantic mask generation. DiffuMask uses text-guided cross-attention information to localize class/word-specific regions, which are combined with practical techniques to create a novel high-resolution and class-discriminative pixel-wise mask. The methods help to reduce data collection and annotation costs obviously. Experiments demonstrate that the existing segmentation methods trained on synthetic data of DiffuMask can achieve a competitive performance over the counterpart of real data (VOC 2012, Cityscapes). For some classes (e.g., bird), DiffuMask presents promising performance, close to the stateof-the-art result of real data (within 3% mIoU gap). Moreover, in the open-vocabulary segmentation (zero-shot) setting, DiffuMask achieves a new SOTA result on Unseen class of VOC 2012.
              </font></div>
              <div id="DiffuMask_bib" style="font-family:Courier;display:none;min-width:350px;"><font size="2">
                <br>
                @InProceedings{wu2023DiffuMask,<br>
                author = {Wu, Weijia and Yuzhong, Zhao and Mike Zheng, Shou and Hong Zhou and Shen, Chunhua},<br>
                title = {DiffuMask: Synthesizing Images with Pixel-level Annotations for Semantic Segmentation Using Diffusion Models},<br>
                booktitle = {arxiv},<br>
                year = {2023}<br>
                }
              </font></div>
              </td>
          </tr>
          
          
          
          
        <table style="width:85%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
            <tr>
            <td style="padding:0px;width:100%;vertical-align:middle;text-align:justify">
               <p>
              </p>
              <heading><font size="5">2022</font></heading>
            </td>
          </tr>
        </tbody></table>

        <table style="width:80%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
		
		<!-- ECCV2022 TransDETR -->
          <tr onmouseout="ECCV2022_stop()" onmouseover="ECCV2022_start()">
            <td style="padding:20px;width:25%;vertical-align:middle">
              <div class="one">
                <div class="two" id='eccv2022'><img src='images/eccv2022.jpg'></div>
				<img src='images/eccv2022.jpg'>
              </div>
              <script type="text/javascript">
                function ECCV2022_start() {
                  document.getElementById('neurips2021').style.opacity = "1";
                }

                function ECCV2022_stop() {
                  document.getElementById('neurips2021').style.opacity = "0";
                }
                ECCV2022_stop()
              </script>
            </td>
            <td style="padding:10px;width:75%;vertical-align:middle">
              <a href="https://arxiv.org/abs/2203.10539" target="_blank">
                <papertitle><font size="3">End-to-End Video Text Spotting with Transformer</font></papertitle>
              </a>
              <br><br>
               <strong><font size="3"> Weijia Wu</font></strong>, <font size="3">Yuanqiang Cai</font>, <font size="3"> Chunhua Shen</font>, <font size="3"> Debing Zhang</font>, <font size="3">  Ying Fu</font>, <font size="3">  Ping Luo</font>,
                <font size="3"> Hong Zhou</font> <font size="3"></font>.
              <br><font size="3">
							<em>International Journal of Computer Vision (<strong><font size="3.5">IJCV</font></strong>)</em>,
              </font>
              <br>
              <p></p>
              <a href="javascript:void(0);" onclick="myFunction('ECCV2022_abs')"><font size="3">Abstract</font></a> /
             <a href="https://arxiv.org/abs/2203.10539" target="_blank" target="_blank"><font size="3">arXiv</font></a> /
              <a href="https://link.springer.com/article/10.1007/s11263-024-02063-1?utm_source=rct_congratemailt&utm_medium=email&utm_campaign=nonoa_20240712&utm_content=10.1007/s11263-024-02063-1" target="_blank" target="_blank"><font size="3">IJCV</font></a> /
              <a href="javascript:void(0);" onclick="myFunction('ECCV2022_bib')"><font size="3">BibTex</font></a> /
			  <a href="https://github.com/weijiawu/TransDETR" target="_blank" target="_blank"><font size="3">GitHub</font></a> /
			  <a href="https://www.youtube.com/watch?v=YpJEGpcxbIQ" target="_blank" target="_blank"><font size="3">Youtube Demo</font></a> /
			  <a href="https://zhuanlan.zhihu.com/p/518894764" target="_blank" target="_blank"><font size="3">ZhiHu</font></a> /
              <p></p>
              <div id="ECCV2022_abs" style="display:none; text-align:justify;min-width:350px;"><font size="3">
                <em> Video text spotting(VTS) is the task that requires simultaneously detecting, tracking and recognizing text in video. Recent methods typically follow tracking-by-match paradigm and develop sophisticated pipelines to tackle this task. 
				In this paper, rooted in Transformer sequence modeling, we propose a simple, but effective end-to-end video text DEtection, Tracking, and Recognition framework (TransDETR). TransDETR mainly includes two advantages: 1) Different from the explicit match paradigm in the adjacent frame, 
				TransDETR tracks and recognizes each text implicitly by the different query termed `text query' over long-range temporal sequence (more than 7 frames). 2) TransDETR is the first end-to-end trainable video text spotting framework, which simultaneously addresses the three sub-tasks (text detection, tracking, recognition). 
				Extensive experiments on four video text datasets (ICDAR2013 Video, ICDAR2015 Video, Minetto, and YouTube Video Text) are conducted to demonstrate that \detr achieves the state-of-the-art performance with up to around 7.0% improvements on detection, tracking, and spotting tasks.
              </font></div>
              <div id="ECCV2022_bib" style="font-family:Courier;display:none;min-width:350px;"><font size="2">
                <br>
                @InProceedings{wu2022end,<br>
                author = {Weijia Wu, Yuanqiang Cai, Chunhua Shen, Debing Zhang, Ying Fu, Ping Luo, Hong Zhou},<br>
                title = {End-to-End Video Text Spotting with Transformer},<br>
                booktitle = {International Journal of Computer Vision},<br>
                year = {2024}<br>
                }
              </font></div>
              </td>
          </tr>
		  
		

		<!-- ECCV2022 CoText -->
          <tr onmouseout="ECCV2022_1_stop()" onmouseover="ECCV2022_1_start()">
            <td style="padding:20px;width:25%;vertical-align:middle">
              <div class="one">
                <div class="two" id='eccv2022_1'><img src='images/ECCV2022_1.JPG'></div>
				<img src='images/ECCV2022_1.JPG'>
              </div>
              <script type="text/javascript">
                function ECCV2022_1_start() {
                  document.getElementById('neurips2021').style.opacity = "1";
                }

                function ECCV2022_1_stop() {
                  document.getElementById('neurips2021').style.opacity = "0";
                }
                ECCV2022_1_stop()
              </script>
            </td>
            <td style="padding:10px;width:75%;vertical-align:middle">
              <a href="https://arxiv.org/abs/2203.10539" target="_blank">
                <papertitle><font size="3">Real-time End-to-End Video Text Spotter with Contrastive Representation Learning</font></papertitle>
              </a>
              <br><br>
               <strong><font size="3"> Weijia Wu</font></strong>, <font size="3">Zhuang Li</font>, <font size="3">Jiahong Li</font>, <font size="3"> Chunhua Shen</font>, <font size="3">  Size Li</font>, <font size="3">  Zhongyuan Wang</font>, <font size="3">  Ping Luo</font>,
                <font size="3"> Hong Zhou</font> <font size="3"></font>.
              <br><font size="3">
							<em>arxiv (<strong><font size="3.5">arxiv</font></strong>)</em>,
              </font>
              <br>
              <p></p>
              <a href="javascript:void(0);" onclick="myFunction('ECCV2022_1_abs')"><font size="3">Abstract</font></a> /
             <a href="https://arxiv.org/abs/2207.08417" target="_blank" target="_blank"><font size="3">arXiv</font></a> /
              <a href="javascript:void(0);" onclick="myFunction('ECCV2022_1_bib')"><font size="3">BibTex</font></a> /
			  <a href="https://github.com/weijiawu/CoText" target="_blank" target="_blank"><font size="3">Code</font></a> /
			  <a href="https://www.youtube.com/watch?v=zXgEzWhfGBM" target="_blank" target="_blank"><font size="3">Youtube Demo</font></a> /
              <p></p>
              <div id="ECCV2022_1_abs" style="display:none; text-align:justify;min-width:350px;"><font size="3">
                <em> Video text spotting(VTS) is the task that requires simultaneously detecting, tracking and recognizing text instances in the video. Existing video text spotting methods typically develop sophisticated pipelines and multiple models, which is no friend for real-time applications. Here we propose a real-time end-to-end video text spotter with Contrastive Representation learning (CoText). Our contributions are four-fold: 1) For the first time, we simultaneously address the three tasks (e.g., text detection, tracking, recognition) in a real-time end-to-end trainable framework.
				2) Like humans, CoText tracks and recognizes texts by comprehending them, relating them each other with visual and semantic representations. 3) With contrastive learning, CoText models long-range dependencies and learning temporal information across multiple frames. 4) A simple, light-weight architecture is designed for effective and accurate performance, including GPU-parallel detection post-processing, CTCbased recognition head with Masked RoI, and track head with contrastive learning. Extensive experiments show the superiority of our method. Especially, CoText achieves an video text spotting IDF1 of 72.0% at 35.2 FPS on ICDAR2015video, with 10.5% and 26.2 FPS improvement
				the previous best method.
              </font></div>
              <div id="ECCV2022_1_bib" style="font-family:Courier;display:none;min-width:350px;"><font size="2">
                <br>
                @InProceedings{wu2022cotext,<br>
                author = {Wu, Weijia and Li, Zhuang and Li, Jiahong and Shen, Chunhua and Zhou, Hong and Li, Size and Luo, Ping},<br>
                title = {Real-time End-to-End Video Text Spotter with Contrastive Representation Learning},<br>
                booktitle = {arxiv},<br>
                year = {2022}<br>
                }
              </font></div>
              </td>
          </tr>
		  
		<!-- ICIP -->
          <tr onmouseout="ICIP_stop()" onmouseover="ICIP_start()">
            <td style="padding:20px;width:25%;vertical-align:middle">
              <div class="one">
                <div class="two" id='icml2022'><img src='images/ICIP2022.JPG'></div>
				<img src='images/ICIP2022.JPG'>
              </div>
              <script type="text/javascript">
                function ICIP_start() {
                  document.getElementById('neurips2021').style.opacity = "1";
                }

                function ICIP_stop() {
                  document.getElementById('neurips2021').style.opacity = "0";
                }
                ICIP_stop()
              </script>
            </td>
            <td style="padding:10px;width:75%;vertical-align:middle">
              <a href="https://arxiv.org/pdf/2011.13307.pdf" target="_blank">
                <papertitle><font size="3">Polygon-free: Unconstrained Scene Text Detection with Box Annotations</font></papertitle>
              </a>
              <br><br>
               <strong><font size="3"> Weijia Wu</font></strong>, <font size="3">Enze Xie</font>, <font size="3"> Ruimao Zhang</font>, <font size="3"> Wenhai Wang</font>, <font size="3">  Ping Luo</font>
                <font size="3"> Hong Zhou</font> <font size="3"></font>.
              <br><font size="3">
							<em>International Conference on Information Processing (<strong><font size="3.5">ICIP2022</font></strong>)</em>,
              </font>
              <br>
              <p></p>
              <a href="javascript:void(0);" onclick="myFunction('ICIP_abs')"><font size="3">Abstract</font></a> /
             <a href="https://cmsworkshops.com/ICIP2022/Papers/Uploads/Proposals/PaperNum/1868/20220312010953_729952_1868.pdf" target="_blank" target="_blank"><font size="3">arXiv</font></a> /
              <a href="javascript:void(0);" onclick="myFunction('ICIP2022_bib')"><font size="3">BibTex</font></a> /
			  <a href="https://github.com/weijiawu/Polygon-free-Unconstrained-Scene-Text-Detection-with-Box-Annotations" target="_blank" target="_blank"><font size="3">Code</font></a> /
              <p></p>
              <div id="ICIP_abs" style="display:none; text-align:justify;min-width:350px;"><font size="3">
                <em> Although a polygon is a more accurate representation than an upright bounding box for text detection, the annotations of polygons are extremely expensive and challenging.
Unlike existing works that employ fully-supervised training with polygon annotations, this study proposes an unconstrained text detection system termed Polygon-free (PF), in which most existing polygon-based text detectors (e.g.,PSENet [33],DB [16]) are trained with only upright bounding box annotations. Our core idea is to transfer knowledge
from synthetic data to real data to enhance the supervision information of upright bounding boxes. This is made possible with a simple segmentation network, namely Skeleton Attention Segmentation Network (SASN), that includes three vital components (i.e., channel attention, spatial attention
and skeleton attention map) and one soft cross-entropy loss. Experiments demonstrate that the proposed Polygonfree system can combine general detectors (e.g., EAST, PSENet, DB) to yield surprisingly high-quality pixel-level results with only upright bounding box annotations on a
variety of datasets (e.g., ICDAR2019-Art, TotalText, ICDAR2015). For example, without using polygon annotations, PSENet achieves an 80.5% F-score on TotalText [3](vs. 80.9% of fully supervised counterpart), 31.1% better than training directly with upright bounding box annotations, and saves 80%+ labeling costs. We hope that
PF can provide a new perspective for text detection to reduce the labeling costs.
              </font></div>
              <div id="ICIP2022_bib" style="font-family:Courier;display:none;min-width:350px;"><font size="2">
                <br>
                @InProceedings{wu2022polygon,<br>
                author = {Weijia Wu and Enze Xie and Ruimao Zhang and Wenhai Wang and Ping Luo and Hong Zhou},<br>
                title = {Polygon-free: Unconstrained Scene Text Detection with Box Annotations},<br>
                booktitle = {IInternational Conference on Information Processing},<br>
                year = {2022}<br>
                }
              </font></div>
              </td>
          </tr>
		  
		  
		<!-- ICML workshop -->
          <tr onmouseout="ICMLworkshop_stop()" onmouseover="ICMLworkshop_start()">
            <td style="padding:20px;width:25%;vertical-align:middle">
              <div class="one">
                <div class="two" id='icml2022'><img src='images/ICML2022.JPG'></div>
				<img src='images/ICML2022.JPG'>
              </div>
              <script type="text/javascript">
                function ICMLworkshop_start() {
                  document.getElementById('neurips2021').style.opacity = "1";
                }

                function ICMLworkshop_stop() {
                  document.getElementById('neurips2021').style.opacity = "0";
                }
                ICMLworkshop_stop()
              </script>
            </td>
            <td style="padding:10px;width:75%;vertical-align:middle">
              <a href="https://openreview.net/pdf?id=5IFFLY4JR24" target="_blank">
                <papertitle><font size="3">ECLIP: Efficient Contrastive Language-Image Pretraining via Ensemble Confidence Learning and Masked Language Modeling</font></papertitle>
              </a>
              <br><br>
               <font size="3">Jue Wang</font>, <font size="3"> Haofan Wang</font>, <strong><font size="3"> Weijia Wu</font></strong>,   <font size="3"> Jincan Deng</font>
                <font size="3">Debing Zhang</font> <font size="3"></font>.
              <br><font size="3">
                            <em>ICML 2022 Pre-training Workshop</em>,2022 </strong></font>
              </font>
              <br>
              <p></p>
              <a href="javascript:void(0);" onclick="myFunction('ICML2021_abs')"><font size="3">Abstract</font></a> /
             <a href="https://arxiv.org/abs/2109.04699" target="_blank" target="_blank"><font size="3">arXiv</font></a> /
              <a href="javascript:void(0);" onclick="myFunction('ICML2022_bib')"><font size="3">BibTex</font></a> /
              <p></p>
              <div id="ICML2021_abs" style="display:none; text-align:justify;min-width:350px;"><font size="3">
                <em> While large scale pre-training has achieved great achievements in bridging the gap between vision and language, it still faces three challenges. First, the cost for pre-training is expensive. Second, there is no efficient way to handle the data noise which degrades model performance. Third,
previous methods only leverage limited imagetext paired data, while ignoring richer singlemodal data, which may result in poor generalization to single-modal downstream tasks. In this
work, we propose Efficient Contrastive LanguageImage Pretraining (ECLIP) via Ensemble Confidence Learning and Masked Language Modeling. Specifically, We adaptively filter out noisy samples in the training process by means of Ensemble
Confidence Learning strategy, and add a Masked Language Modeling objective to utilize extra nonpaired text data. ECLIP achieves the state-of-theart performance on Chinese cross-modal retrieval
tasks with only 1/10 training resources compared with CLIP and WenLan, while showing excellent generalization to single-modal tasks including text retrieval and text classification.
              </font></div>
              <div id="ICML2022_bib" style="font-family:Courier;display:none;min-width:350px;"><font size="2">
                <br>
                @InProceedings{wang2021efficientclip,<br>
                author = {Wang, Jue and Wang, Haofan and Wu, Weijia and Deng, Jincan and Zhang, Debing},<br>
                title = {Efficientclip: Efficient cross-modal pre-training by ensemble confident learning and language modeling},<br>
                booktitle = {ICML 2022 Pre-training Workshop},<br>
                year = {2022}<br>
                }
              </font></div>
              </td>
          </tr>
		  
		  
		  
		
		<table style="width:85%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
            <tr>
            <td style="padding:0px;width:100%;vertical-align:middle;text-align:justify">
               <p>
              </p>
              <heading><font size="5">2021</font></heading>
            </td>
          </tr>
        </tbody></table>

        <table style="width:80%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
		

         <!-- ICCV - HANDWRITING RECOGNITION -->
          <tr onmouseout="neurips2021_stop()" onmouseover="neurips2021_start()">
            <td style="padding:20px;width:25%;vertical-align:middle">
              <div class="one">
                <div class="two" id='neurips2021'><img src='images/neurips2021.jpg'></div>
				<img src='images/neurips2021.jpg'>
              </div>
              <script type="text/javascript">
                function neurips2021_start() {
                  document.getElementById('neurips2021').style.opacity = "1";
                }

                function neurips2021_stop() {
                  document.getElementById('neurips2021').style.opacity = "0";
                }
                neurips2021_stop()
              </script>
            </td>
            <td style="padding:10px;width:75%;vertical-align:middle">
              <a href="https://arxiv.org/pdf/2112.04888.pdf" target="_blank">
                <papertitle><font size="3">A Bilingual, Open World Video Text Dataset and End-to-end Video Text Spotter with Transformer</font></papertitle>
              </a>
              <br><br>
               <strong><font size="3"> Weijia Wu</font></strong>, <font size="3">Yuanqiang Cai</font>, <font size="3">Debing Zhang</font>, <font size="3">Jiahong Li</font>
                <font size="3">Hong Zhou</font> <font size="3"></font>.
              <br><font size="3">
                            <em>NeurIPS 2021 Track on Datasets and Benchmarks (<strong><font size="3.5">NeurIPS</font></strong>)</em>,
              2021 
              </font>
              <br>
              <p></p>
              <a href="javascript:void(0);" onclick="myFunction('neurips2021_abs')"><font size="3">Abstract</font></a> /
             <a href="https://arxiv.org/abs/2112.04888" target="_blank" target="_blank"><font size="3">arXiv</font></a> /
			 <a href="https://sites.google.com/view/video-and-langauge/bovtext?authuser=0" target="_blank" target="_blank"><font size="3">Homepage</font></a> /
              <a href="javascript:void(0);" onclick="myFunction('neurips2021_bib')"><font size="3">BibTex</font></a> /
			  <a href="https://github.com/weijiawu/BOVText-Benchmark" target="_blank" target="_blank"><font size="3">GitHub</font></a>/
             <a href="https://www.youtube.com/watch?v=mS66yr1WmI4" target="_blank" target="_blank"><font size="3">Demo</font></a> /
             <a href="https://mp.weixin.qq.com/s/BPXFyYn6IfuS-f9iE8YYFw" target="_blank" target="_blank"><font size="3">中文公众号报道</font></a> /
             <a href="https://zhuanlan.zhihu.com/p/514393835" target="_blank" target="_blank"><font size="3">感谢其他人的知乎解读</font></a>
              <p></p>
              <div id="neurips2021_abs" style="display:none; text-align:justify;min-width:350px;"><font size="3">
                <em> Most existing video text spotting benchmarks focus on evaluating a single language and scenario with limited data. In this work, we introduce a large-scale, Bilingual, Open World Video text benchmark dataset(BOVText). There are four features for BOVText. Firstly, we provide 2,000+ videos with more than 1,750,000+ frames, 25 times larger than the existing largest dataset with incidental text in videos. Secondly, our dataset covers 30+ open categories with a wide selection of various scenarios, e.g., Life Vlog, Driving, Movie, etc. Thirdly, abundant text types annotation (i.e., title, caption or scene text) are provided for the different representational meanings in video. Fourthly, the BOVText provides bilingual text annotation to promote multiple cultures live and communication. Besides, we propose an end-to-end video text spotting framework with Transformer, termed TransVTSpotter, which solves the multi-orient text spotting in video with a simple, but efficient attention-based query-key mechanism. It applies object features from the previous frame as a tracking query for the current frame and introduces a rotation angle prediction to fit the multiorient text instance. On ICDAR2015(video), TransVTSpotter achieves the state-of-the-art performance with 44.1% MOTA, 9 fps.
              </font></div>
              <div id="neurips2021_bib" style="font-family:Courier;display:none;min-width:350px;"><font size="2">
                <br>
                @InProceedings{wu2021opentext,<br>
                author = {Weijia Wu, Debing Zhang, Yuanqiang Cai, Sibo Wang, Jiahong Li, Zhuang Li, Yejun Tang, Hong Zhou},<br>
                title = {A Bilingual, OpenWorld Video Text Dataset and End-to-end Video Text Spotter with Transformer},<br>
                booktitle = {35th Conference on Neural Information Processing Systems (NeurIPS 2021) Track on Datasets and Benchmarks},<br>
                year = {2021}<br>
                }
              </font></div>
              </td>
          </tr>
		  
		   <table style="width:85%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
            <tr>
            <td style="padding:0px;width:100%;vertical-align:middle;text-align:justify">
               <p>
              </p>
              <heading><font size="5">2020</font></heading>
            </td>
          </tr>
        </tbody></table>

        <table style="width:80%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>


         <!-- ACCV - HANDWRITING RECOGNITION -->
          <tr onmouseout="accv2020_stop()" onmouseover="accv2020_start()">
            <td style="padding:20px;width:25%;vertical-align:middle">
              <div class="one">
                <div class="two" id='accv2020'><img src='images/accv2020.jpg'></div>
				<img src='images/accv2020.jpg'>
              </div>
              <script type="text/javascript">
                function accv2020_start() {
                  document.getElementById('accv2020').style.opacity = "1";
                }

                function accv2020_stop() {
                  document.getElementById('accv2020').style.opacity = "0";
                }
                accv2020_stop()
              </script>
            </td>
            <td style="padding:10px;width:75%;vertical-align:middle">
              <a href="https://openaccess.thecvf.com/content/ACCV2020/papers/Wu_Synthetic-to-Real_Unsupervised_Domain_Adaptation_for_Scene_Text_Detection_in_the_ACCV_2020_paper.pdf" target="_blank">
                <papertitle><font size="3">Synthetic-to-Real Unsupervised Domain Adaptation for Scene Text Detection in the Wild</font></papertitle>
              </a>
              <br><br>
               <strong><font size="3"> Weijia Wu</font></strong>, <font size="3">Ning Lu</font>, <font size="3">Enze Xie</font>,
                <font size="3">Hong Zhou</font> <font size="3"></font>.
              <br><font size="3">
                            <em>Proceedings of the Asian Conference on Computer Vision (<strong><font size="3.5">ACCV</font></strong>)</em>,
              2020.
              </font>
              <br>
              <p></p>
              <a href="javascript:void(0);" onclick="myFunction('accv2020_abs')"><font size="3">Abstract</font></a> /
             <a href="https://arxiv.org/abs/2009.01766" target="_blank" target="_blank"><font size="3">arXiv</font></a> /
              <a href="javascript:void(0);" onclick="myFunction('accv2020_bib')"><font size="3">BibTex</font></a>
              <p></p>
              <div id="accv2020_abs" style="display:none; text-align:justify;min-width:350px;"><font size="3">
                <em> Deep learning-based scene text detection can achieve preferable performance, powered with sufficient labeled training data. However, manual labeling is time consuming and laborious. At the extreme, the corresponding annotated data are unavailable. Exploiting synthetic data is a very promising solution except for domain distribution mismatches between synthetic datasets and real datasets. To address the severe domain distribution mismatch, we propose a synthetic-to-real domain adaptation method for scene text detection, which transfers knowledge from synthetic data (source domain) to real data (target domain). In this paper, a text self-training (TST) method and adversarial text instance alignment (ATA) for domain adaptive scene text detection are introduced. ATA helps the network learn domain-invariant features by training a domain classifier in an adversarial manner. TST diminishes the adverse effects of false positives~(FPs) and false negatives~(FNs) from inaccurate pseudo-labels. Two components have positive effects on improving the performance of scene text detectors when adapting from synthetic-to-real scenes. We evaluate the proposed method by transferring from SynthText, VISD to ICDAR2015, ICDAR2013. The results demonstrate the effectiveness of the proposed method with up to 10% improvement, which has important exploration significance for domain adaptive scene text detection.
              </font></div>
              <div id="accv2020_bib" style="font-family:Courier;display:none;min-width:350px;"><font size="2">
                <br>
                @InProceedings{wu2020synthetic,<br>
                author = {Wu, Weijia and Lu, Ning and Xie, Enze and Wang, Yuxing and Yu, Wenwen and Yang, Cheng and Zhou, Hong},<br>
                title = {Synthetic-to-Real Unsupervised Domain Adaptation for Scene Text Detection in the Wild},<br>
                booktitle = {Proceedings of the Asian Conference on Computer Vision (ACCV)},<br>
                year = {2020}<br>
                }
              </font></div>
              </td>
          </tr>
	  
	  
	    
	    <table
            style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;">
			<tbody>
			  <tr>
				<td style="padding:20px;width:100%;vertical-align:middle;text-align:justify">
				
				  <heading>
					  <font size="5">Academic Service</font>
				  </heading>
				  <p>
				  <heading><font size="4">Conference Review</font></heading>
				  <p>
				  <p>
					<font size="3">
							<li><sup></strong></font></sup><font size=3.5">International Conference on Machine Learning(ICML), 2022, 2023, 2024.</li>
							<li><sup></strong></font></sup><font size=3.5">Neural Information Processing Systems(NeurIPS), 2021, 2022, 2023, 2024.</li>
							<li><sup></strong></font></sup><font size=3.5">Track Datasets and Benchmarks of Neural Information Processing Systems(NeurIPS), 2021, 2022, 2023, 2024.</li>
                            <li><sup></strong></font></sup><font size=3.5">The Association for Computational Linguistics (ACL) 2024.</li>
                            <li><sup></strong></font></sup><font size=3.5">International Conference on Learning Representations(ICLR), 2023, 2024, 2025.</li>
                            <li><sup></strong></font></sup><font size=3.5">The Association for the Advancement of Artificial Intelligence(AAAI), 2025.</li>
                            <li><sup></strong></font></sup><font size=3.5">IEEE/CVF Conference on Computer Vision and Pattern Recognition(CVPR), 2024.</li>
                            <li><sup></strong></font></sup><font size=3.5">European Conference on Computer Vision (ECCV), 2024.</li>
                            <li><sup></strong></font></sup><font size=3.5"><a href="https://syntagen.github.io/#syntagen-competition" target="_blank" target="_blank"><font size="3">CVPR 2024 Workshop SyntaGen</font></a>.</li>
				  </p>
				  
				  <heading><font size="4">Journal Review</font></heading>
                  <p>
                <p>
                  <font size="3">
                          <li><sup></strong></font></sup><font size=3.5">IEEE Transactions on Neural Networks and Learning Systems (TNNLS).</li>
                          <li><sup></strong></font></sup><font size=3.5">IEEE Transactions on Circuits and Systems for Video Technology (TCSVT).</li>
                          <li><sup></strong></font></sup><font size=3.5">International Journal of Computer Vision (IJCV).</li>
                          <li><sup></strong></font></sup><font size=3.5">Transactions on Multimedia Computing Communications and Applications (ACM TOMM).</li>
                </p>
				  </font>
				  </p>
				</td>
			  </tr>
			</tbody>
			
            <tbody>
              <tr>
                <td style="padding:20px;width:100%;vertical-align:middle;text-align:justify">
                
                  <heading>
                      <font size="5">Awards</font>
                  </heading>
                  <p>
                    <font size="3">
                            <li><sup></strong></font></sup><font size=3.5">[10/2020] National Scholarship of China.</li>
                  </p>
                </td>
              </tr>
            </tbody>
			
			<tbody>
			  <tr>
				<td style="padding:20px;width:100%;vertical-align:middle;text-align:justify">
				
				  <heading>
					  <font size="5">Talks</font>
				  </heading>
				  <p>
					<font size="3">
							<li><sup></strong></font></sup><font size=3.5">[10/2021] MMU, Kuaishou. <a href="https://drive.google.com/file/d/1RfZVNEa_i0v0cd9t1igQwjgWgzuu1mGn/view?usp=sharing">Video Text Spotting</a>.</li>
				  </p>
				</td>
			  </tr>
			</tbody>
		
        </table>

		  
      <table align=center width=600px>
      <tr><td align=center width=600px>
          <center><img src = "./images/logo.png" height="75x"></img><br></center>

      </td></tr>
      </table>

        <!-- Footer - Template Credits -->
        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
          <tr>
            <td style="padding:0px">
              <br>
              <p style="text-align:center;font-size:small;">
                Template credits :
                <a href="https://jonbarron.info/" target="_blank">Dr. Jon Barron</a>
              </p>
            </td>
          </tr>
        </tbody></table>

      </td>
    </tr>
  </table>

        <table align=center width=300px>
      <tr><td align=center width=300px>
          <script type="text/javascript" id="clustrmaps" src="//clustrmaps.com/map_v2.js?d=3ggBviUhnXIQYDHmIz11N6EfhXliwaA0PSN5rvVKWPs&cl=ffffff&w=a"></script>
      </td></tr>
      </table>

<!--        <table align=center width=300px>-->
<!--      <tr><td align=center width=300px>-->
<!--          <script type="application/javascript">-->
<!--    (function(b,o,n,g,s,r,c){if(b[s])return;b[s]={};b[s].scriptToken="XzEwMjgyODE3Mjg";b[s].callsQueue=[];b[s].api=function(){b[s].callsQueue.push(arguments);};r=o.createElement(n);c=o.getElementsByTagName(n)[0];r.async=1;r.src=g;r.id=s+n;c.parentNode.insertBefore(r,c);})(window,document,"script","https://cdn.oribi.io/XzEwMjgyODE3Mjg/oribi.js","ORIBI");-->
<!--    </script>-->
<!--      </td></tr>-->
<!--      </table>-->

</body>

</html>
